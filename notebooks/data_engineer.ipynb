{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statistical analysis\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Machine learning (supervised learning)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_excel(\"2021.xlsx\")\n",
    "df_2 = pd.read_excel(\"2022.xlsx\")\n",
    "df_concatenated = pd.concat([df_1, df_2], ignore_index=True)\n",
    "# Display the first few rows of the concatenated DataFrame\n",
    "print(df_concatenated.head())\n",
    "# Display the shape of the concatenated DataFrame\n",
    "print(f\"Shape of concatenated DataFrame: {df_concatenated.shape}\")\n",
    "# Display the columns of the concatenated DataFrame\n",
    "print(f\"Columns in concatenated DataFrame: {df_concatenated.columns.tolist()}\")\n",
    "# Display the data types of the columns in the concatenated DataFrame\n",
    "print(f\"Data types of columns in concatenated DataFrame:\\n{df_concatenated.dtypes}\")\n",
    "# Display summary statistics of the concatenated DataFrame\n",
    "print(f\"Summary statistics of concatenated DataFrame:\\n{df_concatenated.describe()}\")\n",
    "# Check for missing values in the concatenated DataFrame\n",
    "print(f\"Missing values in concatenated DataFrame:\\n{df_concatenated.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing and Initial Exploration\n",
    "# Building on your concatenated dataset\n",
    "\n",
    "# First, let's standardize column names to English for easier processing\n",
    "df_concatenated.columns = ['sales_month', 'primary_platform', 'secondary_platform', \n",
    "                          'store_name', 'brand_name', 'product_code', \n",
    "                          'sales_amount', 'sales_quantity']\n",
    "\n",
    "platform_mapping = {\n",
    "    '抖音': 'Douyin',\n",
    "    '京东': 'JD',\n",
    "    '天猫': 'Tmall'\n",
    "}\n",
    "# Map primary_platform to English names\n",
    "df_concatenated['primary_platform'] = df_concatenated['primary_platform'].map(platform_mapping)\n",
    "\n",
    "# Convert sales_month to datetime\n",
    "df_concatenated['sales_month'] = pd.to_datetime(df_concatenated['sales_month'])\n",
    "\n",
    "# Add year column for analysis\n",
    "df_concatenated['year'] = df_concatenated['sales_month'].dt.year\n",
    "\n",
    "print(\"=== BASIC DATA PREPROCESSING ===\")\n",
    "print(f\"Date range: {df_concatenated['sales_month'].min()} to {df_concatenated['sales_month'].max()}\")\n",
    "print(f\"Years covered: {sorted(df_concatenated['year'].unique())}\")\n",
    "print(f\"Platforms: {df_concatenated['primary_platform'].unique()}\")\n",
    "\n",
    "# Platform distribution analysis\n",
    "print(\"\\n=== PLATFORM COVERAGE ANALYSIS ===\")\n",
    "platform_coverage = df_concatenated.groupby(['primary_platform', 'year']).agg({\n",
    "    'sales_month': ['min', 'max', 'nunique'],\n",
    "    'sales_quantity': ['count', 'sum'],\n",
    "    'store_name': 'nunique',\n",
    "    'brand_name': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "platform_coverage.columns = ['_'.join(col) for col in platform_coverage.columns]\n",
    "print(platform_coverage)\n",
    "\n",
    "# Missing data analysis\n",
    "print(\"\\n=== MISSING DATA ANALYSIS ===\")\n",
    "missing_by_platform = df_concatenated.groupby('primary_platform').apply(\n",
    "    lambda x: x.isnull().sum()\n",
    ")\n",
    "print(\"Missing data by platform:\")\n",
    "print(missing_by_platform)\n",
    "\n",
    "# Handle missing values strategically\n",
    "print(\"\\n=== MISSING DATA HANDLING ===\")\n",
    "# For missing store names, we'll create a placeholder\n",
    "df_concatenated['store_name'] = df_concatenated['store_name'].fillna('Unknown_Store')\n",
    "\n",
    "# For missing brand names, let's analyze the pattern first\n",
    "missing_brands = df_concatenated[df_concatenated['brand_name'].isnull()]\n",
    "print(f\"Records with missing brand names: {len(missing_brands)}\")\n",
    "print(\"Platform distribution of missing brands:\")\n",
    "print(missing_brands['primary_platform'].value_counts())\n",
    "\n",
    "# Fill missing brand names with 'Unknown_Brand'\n",
    "df_concatenated['brand_name'] = df_concatenated['brand_name'].fillna('Unknown_Brand')\n",
    "\n",
    "print(f\"After handling missing data: {df_concatenated.isnull().sum().sum()} missing values remain\")\n",
    "\n",
    "# Data quality checks\n",
    "print(\"\\n=== DATA QUALITY ANALYSIS ===\")\n",
    "\n",
    "# Check for zero/negative sales\n",
    "zero_sales = df_concatenated[df_concatenated['sales_quantity'] <= 0]\n",
    "print(f\"Records with zero or negative sales quantity: {len(zero_sales)} ({len(zero_sales)/len(df_concatenated)*100:.2f}%)\")\n",
    "\n",
    "zero_amount = df_concatenated[df_concatenated['sales_amount'] <= 0]\n",
    "print(f\"Records with zero or negative sales amount: {len(zero_amount)} ({len(zero_amount)/len(df_concatenated)*100:.2f}%)\")\n",
    "\n",
    "# Price analysis (sales_amount / sales_quantity)\n",
    "df_concatenated['unit_price'] = df_concatenated['sales_amount'] / df_concatenated['sales_quantity']\n",
    "df_concatenated['unit_price'] = df_concatenated['unit_price'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "print(f\"\\nUnit price statistics:\")\n",
    "print(df_concatenated['unit_price'].describe())\n",
    "\n",
    "# Extreme value analysis\n",
    "print(\"\\n=== EXTREME VALUES ANALYSIS ===\")\n",
    "q99 = df_concatenated['sales_quantity'].quantile(0.99)\n",
    "q95 = df_concatenated['sales_quantity'].quantile(0.95)\n",
    "q90 = df_concatenated['sales_quantity'].quantile(0.90)\n",
    "\n",
    "print(f\"Sales quantity percentiles:\")\n",
    "print(f\"90th percentile: {q90:,.0f}\")\n",
    "print(f\"95th percentile: {q95:,.0f}\")\n",
    "print(f\"99th percentile: {q99:,.0f}\")\n",
    "print(f\"Maximum: {df_concatenated['sales_quantity'].max():,.0f}\")\n",
    "\n",
    "# Identify extreme outliers (>99th percentile)\n",
    "extreme_outliers = df_concatenated[df_concatenated['sales_quantity'] > q99]\n",
    "print(f\"\\nExtreme outliers (>99th percentile): {len(extreme_outliers)} records\")\n",
    "print(\"Top 5 extreme sales:\")\n",
    "print(extreme_outliers.nlargest(5, 'sales_quantity')[['sales_month', 'primary_platform', 'store_name', 'brand_name', 'sales_quantity']])\n",
    "\n",
    "# Monthly sales distribution\n",
    "print(\"\\n=== TEMPORAL DISTRIBUTION ANALYSIS ===\")\n",
    "monthly_totals = df_concatenated.groupby(['year', df_concatenated['sales_month'].dt.month]).agg({\n",
    "    'sales_quantity': 'sum',\n",
    "    'sales_amount': 'sum'\n",
    "}).round(0)\n",
    "\n",
    "monthly_totals.columns = ['total_quantity', 'total_amount']\n",
    "print(\"Monthly sales totals by year:\")\n",
    "print(monthly_totals.head(10))\n",
    "\n",
    "# Platform comparison\n",
    "print(\"\\n=== PLATFORM PERFORMANCE COMPARISON ===\")\n",
    "platform_stats = df_concatenated.groupby('primary_platform').agg({\n",
    "    'sales_quantity': ['count', 'sum', 'mean', 'std', 'min', 'max'],\n",
    "    'sales_amount': ['sum', 'mean'],\n",
    "    'store_name': 'nunique',\n",
    "    'brand_name': 'nunique',\n",
    "    'unit_price': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "platform_stats.columns = ['_'.join(col) for col in platform_stats.columns]\n",
    "print(platform_stats)\n",
    "\n",
    "# Save processed data for next steps\n",
    "print(\"\\n=== DATA READY FOR FEATURE ENGINEERING ===\")\n",
    "print(f\"Final dataset shape: {df_concatenated.shape}\")\n",
    "print(f\"Date range: {df_concatenated['sales_month'].min()} to {df_concatenated['sales_month'].max()}\")\n",
    "print(f\"Total platforms: {df_concatenated['primary_platform'].nunique()}\")\n",
    "print(f\"Total stores: {df_concatenated['store_name'].nunique()}\")\n",
    "print(f\"Total brands: {df_concatenated['brand_name'].nunique()}\")\n",
    "print(f\"Total products: {df_concatenated['product_code'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Advanced Temporal Feature Engineering\n",
    "# Building sophisticated time-based features to capture customer behavior and seasonal patterns\n",
    "\n",
    "def create_advanced_temporal_features(df):\n",
    "    \"\"\"\n",
    "    Create advanced temporal features based on the patterns observed in preprocessing\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"=== CREATING ADVANCED TEMPORAL FEATURES ===\")\n",
    "    \n",
    "    # Sort data properly for time series operations\n",
    "    df = df.sort_values(['primary_platform', 'store_name', 'brand_name', 'sales_month'])\n",
    "    \n",
    "    # Basic temporal components\n",
    "    df['month'] = df['sales_month'].dt.month\n",
    "    df['quarter'] = df['sales_month'].dt.quarter\n",
    "    df['year'] = df['sales_month'].dt.year\n",
    "    df['month_year'] = df['sales_month'].dt.to_period('M')\n",
    "    \n",
    "    # Days since start of dataset (for trend analysis)\n",
    "    start_date = df['sales_month'].min()\n",
    "    df['days_since_start'] = (df['sales_month'] - start_date).dt.days\n",
    "    \n",
    "    print(\"✓ Basic temporal components created\")\n",
    "    \n",
    "    # Cyclical encoding for temporal features (crucial for neural networks)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['quarter_sin'] = np.sin(2 * np.pi * df['quarter'] / 4)\n",
    "    df['quarter_cos'] = np.cos(2 * np.pi * df['quarter'] / 4)\n",
    "    \n",
    "    print(\"✓ Cyclical encoding applied\")\n",
    "    \n",
    "    # Chinese e-commerce calendar features (based on your domain knowledge)\n",
    "    promotional_months = [1, 6, 9, 11, 12]  # From your previous analysis\n",
    "    \n",
    "    df['is_promotional'] = df['month'].isin(promotional_months).astype(int)\n",
    "    \n",
    "    # Distance to promotional periods (helps model anticipate events)\n",
    "    def calculate_distance_to_events(month, event_months):\n",
    "        if not event_months:\n",
    "            return 6  # Maximum distance\n",
    "        distances = []\n",
    "        for event_month in event_months:\n",
    "            # Calculate circular distance (considering year wraparound)\n",
    "            distance = min(abs(month - event_month), 12 - abs(month - event_month))\n",
    "            distances.append(distance)\n",
    "        return min(distances)\n",
    "    \n",
    "    df['distance_to_promo'] = df['month'].apply(lambda x: calculate_distance_to_events(x, promotional_months))\n",
    "    \n",
    "    print(\"✓ Chinese e-commerce calendar features created\")\n",
    "    \n",
    "    # Data-driven seasonal intensity (learning from actual data patterns)\n",
    "    monthly_baseline = df.groupby('month')['sales_quantity'].mean()\n",
    "    overall_mean = df['sales_quantity'].mean()\n",
    "    monthly_intensity = (monthly_baseline / overall_mean).to_dict()\n",
    "    \n",
    "    df['monthly_intensity_learned'] = df['month'].map(monthly_intensity)\n",
    "    \n",
    "    # Platform-specific seasonal patterns\n",
    "    platform_monthly_patterns = df.groupby(['primary_platform', 'month'])['sales_quantity'].mean()\n",
    "    platform_overall_means = df.groupby('primary_platform')['sales_quantity'].mean()\n",
    "    \n",
    "    def get_platform_seasonal_index(row):\n",
    "        platform = row['primary_platform']\n",
    "        month = row['month']\n",
    "        pattern_value = platform_monthly_patterns.get((platform, month), platform_overall_means.get(platform, overall_mean))\n",
    "        baseline = platform_overall_means.get(platform, overall_mean)\n",
    "        return pattern_value / baseline if baseline > 0 else 1\n",
    "    \n",
    "    df['platform_seasonal_index'] = df.apply(get_platform_seasonal_index, axis=1)\n",
    "    \n",
    "    print(\"✓ Data-driven seasonal patterns learned\")\n",
    "    \n",
    "    # Year-over-year growth features\n",
    "    df['is_2022'] = (df['year'] == 2022).astype(int)\n",
    "    \n",
    "    # Create year-over-year comparison features\n",
    "    yoy_comparison = df.groupby(['primary_platform', 'store_name', 'brand_name', 'month']).agg({\n",
    "        'sales_quantity': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    yoy_2021 = yoy_comparison[yoy_comparison['month'].isin([4,5,6,7,8,9,10,11,12])].copy()  # Douyin available months\n",
    "    yoy_2021['comparison_key'] = yoy_2021['primary_platform'] + '_' + yoy_2021['store_name'] + '_' + yoy_2021['brand_name'] + '_' + yoy_2021['month'].astype(str)\n",
    "    yoy_baseline = yoy_2021.set_index('comparison_key')['sales_quantity'].to_dict()\n",
    "    \n",
    "    df['comparison_key'] = df['primary_platform'] + '_' + df['store_name'] + '_' + df['brand_name'] + '_' + df['month'].astype(str)\n",
    "    df['yoy_baseline'] = df['comparison_key'].map(yoy_baseline)\n",
    "    df['yoy_growth_potential'] = np.where(\n",
    "        (df['year'] == 2022) & (df['yoy_baseline'].notna()),\n",
    "        df['sales_quantity'] / df['yoy_baseline'],\n",
    "        1\n",
    "    )\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    df = df.drop(['comparison_key', 'yoy_baseline'], axis=1)\n",
    "    \n",
    "    print(\"✓ Year-over-year growth features created\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_customer_behavior_features(df):\n",
    "    \"\"\"\n",
    "    Create features that capture customer purchasing behavior patterns\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"\\n=== CREATING CUSTOMER BEHAVIOR FEATURES ===\")\n",
    "    \n",
    "    # Store-level behavior analysis\n",
    "    store_behavior = df.groupby(['store_name', 'primary_platform']).agg({\n",
    "        'sales_quantity': ['mean', 'std', 'min', 'max', 'count'],\n",
    "        'sales_amount': ['mean', 'sum'],\n",
    "        'unit_price': ['mean', 'std'],\n",
    "        'brand_name': 'nunique',\n",
    "        'product_code': 'nunique'\n",
    "    })\n",
    "    \n",
    "    # Flatten column names\n",
    "    store_behavior.columns = ['_'.join(col) for col in store_behavior.columns]\n",
    "    store_behavior = store_behavior.reset_index()\n",
    "    \n",
    "    # Create derived behavior metrics\n",
    "    store_behavior['store_sales_cv'] = (\n",
    "        store_behavior['sales_quantity_std'] / store_behavior['sales_quantity_mean']\n",
    "    ).fillna(0)  # Coefficient of variation - measures consistency\n",
    "    \n",
    "    store_behavior['store_sales_range'] = (\n",
    "        store_behavior['sales_quantity_max'] - store_behavior['sales_quantity_min']\n",
    "    )  # Sales volatility\n",
    "    \n",
    "    store_behavior['avg_revenue_per_transaction'] = (\n",
    "        store_behavior['sales_amount_mean'] / store_behavior['sales_quantity_mean']\n",
    "    ).fillna(0)\n",
    "    \n",
    "    store_behavior['brand_diversity'] = store_behavior['brand_name_nunique']\n",
    "    store_behavior['product_diversity'] = store_behavior['product_code_nunique']\n",
    "    \n",
    "    # Price positioning\n",
    "    store_behavior['price_premium_index'] = (\n",
    "        store_behavior['unit_price_mean'] / store_behavior['unit_price_mean'].mean()\n",
    "    ).fillna(1)\n",
    "    \n",
    "    # Store size categorization based on sales volume\n",
    "    store_behavior['total_historical_sales'] = store_behavior['sales_quantity_count'] * store_behavior['sales_quantity_mean']\n",
    "    store_behavior['store_size_category'] = pd.qcut(\n",
    "        store_behavior['total_historical_sales'], \n",
    "        q=5, \n",
    "        labels=['Micro', 'Small', 'Medium', 'Large', 'Mega'],\n",
    "        duplicates='drop'\n",
    "    )\n",
    "    \n",
    "    # Merge back to main dataset\n",
    "    behavior_cols = ['store_name', 'primary_platform', 'store_sales_cv', 'store_sales_range',\n",
    "                    'avg_revenue_per_transaction', 'brand_diversity', 'product_diversity',\n",
    "                    'price_premium_index', 'store_size_category']\n",
    "    \n",
    "    df = df.merge(store_behavior[behavior_cols], on=['store_name', 'primary_platform'], how='left')\n",
    "    \n",
    "    print(\"✓ Store behavior features created\")\n",
    "    \n",
    "    # Brand market positioning features\n",
    "    brand_positioning = df.groupby(['brand_name', 'primary_platform']).agg({\n",
    "        'sales_quantity': ['mean', 'sum', 'count'],\n",
    "        'unit_price': 'mean',\n",
    "        'store_name': 'nunique'\n",
    "    })\n",
    "    \n",
    "    brand_positioning.columns = ['_'.join(col) for col in brand_positioning.columns]\n",
    "    brand_positioning = brand_positioning.reset_index()\n",
    "    \n",
    "    # Brand market share on each platform\n",
    "    platform_totals = df.groupby('primary_platform')['sales_quantity'].sum()\n",
    "    \n",
    "    def calculate_brand_market_share(row):\n",
    "        platform = row['primary_platform']\n",
    "        brand_total = brand_positioning[\n",
    "            (brand_positioning['brand_name'] == row['brand_name']) & \n",
    "            (brand_positioning['primary_platform'] == platform)\n",
    "        ]['sales_quantity_sum'].iloc[0] if len(brand_positioning[\n",
    "            (brand_positioning['brand_name'] == row['brand_name']) & \n",
    "            (brand_positioning['primary_platform'] == platform)\n",
    "        ]) > 0 else 0\n",
    "        \n",
    "        platform_total = platform_totals.get(platform, 1)\n",
    "        return brand_total / platform_total if platform_total > 0 else 0\n",
    "    \n",
    "    # Simplified market share calculation\n",
    "    brand_market_share = df.groupby(['brand_name', 'primary_platform'])['sales_quantity'].sum().reset_index()\n",
    "    platform_totals_dict = df.groupby('primary_platform')['sales_quantity'].sum().to_dict()\n",
    "    \n",
    "    brand_market_share['brand_market_share'] = brand_market_share.apply(\n",
    "        lambda x: x['sales_quantity'] / platform_totals_dict.get(x['primary_platform'], 1), axis=1\n",
    "    )\n",
    "    \n",
    "    df = df.merge(\n",
    "        brand_market_share[['brand_name', 'primary_platform', 'brand_market_share']], \n",
    "        on=['brand_name', 'primary_platform'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Brand positioning features created\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the feature engineering\n",
    "print(\"Starting Step 2: Advanced Temporal Feature Engineering\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create the features (run this in your notebook)  \n",
    "df_with_temporal = create_advanced_temporal_features(df_concatenated)\n",
    "df_with_behavior = create_customer_behavior_features(df_with_temporal)\n",
    "\n",
    "print(f\"\\nFeature engineering complete!\")\n",
    "print(f\"Original columns: {len(df_concatenated.columns)}\")\n",
    "print(f\"After temporal features: {len(df_with_temporal.columns)}\")\n",
    "print(f\"After behavior features: {len(df_with_behavior.columns)}\")\n",
    "print(f\"New features added: {len(df_with_behavior.columns) - len(df_concatenated.columns)}\")\n",
    "\n",
    "# Display new feature categories\n",
    "temporal_features = [col for col in df_with_behavior.columns if any(x in col.lower() for x in \n",
    "    ['month', 'quarter', 'sin', 'cos', 'promotional', 'festival', 'distance', 'intensity', 'seasonal', 'yoy'])]\n",
    "\n",
    "behavior_features = [col for col in df_with_behavior.columns if any(x in col.lower() for x in \n",
    "    ['store_', 'brand_', 'diversity', 'premium', 'market_share', 'size_category'])]\n",
    "\n",
    "print(f\"\\nTemporal Features ({len(temporal_features)}):\")\n",
    "for feature in temporal_features:\n",
    "    print(f\"  • {feature}\")\n",
    "\n",
    "print(f\"\\nBehavior Features ({len(behavior_features)}):\")  \n",
    "for feature in behavior_features:\n",
    "    print(f\"  • {feature}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ready for Step 3: Lag Features and Temporal Dependencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Advanced Lag Features and Temporal Dependencies\n",
    "# Creating sophisticated time-series features to capture momentum, trends, and temporal patterns\n",
    "\n",
    "def create_lag_and_rolling_features(df):\n",
    "    \"\"\"\n",
    "    Create advanced lag and rolling window features for temporal dependencies\n",
    "    Focuses on store-brand-platform combinations to capture specific patterns\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    print(\"=== CREATING LAG AND ROLLING FEATURES ===\")\n",
    "    \n",
    "    # Ensure proper sorting for time series operations\n",
    "    df = df.sort_values(['primary_platform', 'store_name', 'brand_name', 'sales_month'])\n",
    "    \n",
    "    # Create unique identifier for store-brand-platform combinations\n",
    "    df['store_brand_platform_key'] = (\n",
    "        df['store_name'].astype(str) + '_' + \n",
    "        df['brand_name'].astype(str) + '_' + \n",
    "        df['primary_platform'].astype(str)\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Created store-brand-platform grouping keys\")\n",
    "    \n",
    "    # Lag features - Different time horizons for different business insights\n",
    "    lag_periods = [1, 2, 3, 6, 12]  # 1-12 months back\n",
    "    \n",
    "    for lag in lag_periods:\n",
    "        df[f'sales_lag_{lag}'] = df.groupby('store_brand_platform_key')['sales_quantity'].shift(lag)\n",
    "        print(f\"✓ Created {lag}-month lag features\")\n",
    "    \n",
    "    # Rolling window features - Capture trends and volatility\n",
    "    rolling_windows = [3, 6, 12]  # 3, 6, 12 month windows\n",
    "    \n",
    "    for window in rolling_windows:\n",
    "        # Rolling mean (trend)\n",
    "        df[f'sales_rolling_mean_{window}'] = (\n",
    "            df.groupby('store_brand_platform_key')['sales_quantity']\n",
    "            .rolling(window=window, min_periods=1)\n",
    "            .mean()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        \n",
    "        # Rolling standard deviation (volatility)\n",
    "        df[f'sales_rolling_std_{window}'] = (\n",
    "            df.groupby('store_brand_platform_key')['sales_quantity']\n",
    "            .rolling(window=window, min_periods=1)\n",
    "            .std()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        ).fillna(0)\n",
    "        \n",
    "        # Rolling min and max (range patterns)\n",
    "        df[f'sales_rolling_min_{window}'] = (\n",
    "            df.groupby('store_brand_platform_key')['sales_quantity']\n",
    "            .rolling(window=window, min_periods=1)\n",
    "            .min()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        \n",
    "        df[f'sales_rolling_max_{window}'] = (\n",
    "            df.groupby('store_brand_platform_key')['sales_quantity']\n",
    "            .rolling(window=window, min_periods=1)\n",
    "            .max()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Created {window}-month rolling window features\")\n",
    "    \n",
    "    # Momentum and trend features\n",
    "    print(\"Creating momentum and trend features...\")\n",
    "    \n",
    "    # Percentage change features (growth rates)\n",
    "    for period in [1, 3, 6]:\n",
    "        df[f'sales_pct_change_{period}'] = (\n",
    "            df.groupby('store_brand_platform_key')['sales_quantity']\n",
    "            .pct_change(periods=period)\n",
    "            .fillna(0)\n",
    "        )\n",
    "    \n",
    "    # Momentum indicators (comparing different time horizons)\n",
    "    df['sales_momentum_short'] = np.where(\n",
    "        df['sales_rolling_mean_3'] != 0,\n",
    "        df['sales_rolling_mean_6'] / df['sales_rolling_mean_3'],\n",
    "        1\n",
    "    )  # 6-month trend vs 3-month trend\n",
    "    \n",
    "    df['sales_momentum_long'] = np.where(\n",
    "        df['sales_rolling_mean_6'] != 0,\n",
    "        df['sales_rolling_mean_12'] / df['sales_rolling_mean_6'],\n",
    "        1\n",
    "    )  # 12-month trend vs 6-month trend\n",
    "    \n",
    "    # Acceleration (second derivative - change in growth rate)\n",
    "    df['sales_acceleration'] = (\n",
    "        df['sales_pct_change_1'] - \n",
    "        df.groupby('store_brand_platform_key')['sales_pct_change_1'].shift(1)\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Volatility indicators\n",
    "    df['sales_volatility_ratio'] = np.where(\n",
    "        df['sales_rolling_mean_6'] != 0,\n",
    "        df['sales_rolling_std_6'] / df['sales_rolling_mean_6'],\n",
    "        0\n",
    "    )  # Coefficient of variation over 6 months\n",
    "    \n",
    "    print(\"✓ Created momentum, trend, and volatility features\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_cross_platform_dynamics(df):\n",
    "    \"\"\"\n",
    "    Create features that capture competitive dynamics and cross-platform effects\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    print(\"\\n=== CREATING CROSS-PLATFORM DYNAMICS ===\")\n",
    "    \n",
    "    # Monthly platform competition intensity\n",
    "    monthly_platform_stats = df.groupby(['sales_month', 'primary_platform']).agg({\n",
    "        'brand_name': 'nunique',\n",
    "        'store_name': 'nunique', \n",
    "        'sales_quantity': ['sum', 'mean', 'count']\n",
    "    })\n",
    "    \n",
    "    monthly_platform_stats.columns = ['_'.join(col) for col in monthly_platform_stats.columns]\n",
    "    monthly_platform_stats = monthly_platform_stats.reset_index()\n",
    "    \n",
    "    # Merge competition metrics\n",
    "    df = df.merge(\n",
    "        monthly_platform_stats.rename(columns={\n",
    "            'brand_name_nunique': 'monthly_competing_brands',\n",
    "            'store_name_nunique': 'monthly_competing_stores',\n",
    "            'sales_quantity_sum': 'monthly_platform_total_sales',\n",
    "            'sales_quantity_mean': 'monthly_platform_avg_sales',\n",
    "            'sales_quantity_count': 'monthly_platform_transactions'\n",
    "        }),\n",
    "        on=['sales_month', 'primary_platform'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Brand performance across platforms (for brands present on multiple platforms)\n",
    "    brand_platform_presence = df.groupby('brand_name')['primary_platform'].nunique()\n",
    "    multi_platform_brands = brand_platform_presence[brand_platform_presence > 1].index\n",
    "    \n",
    "    df['is_multi_platform_brand'] = df['brand_name'].isin(multi_platform_brands).astype(int)\n",
    "    \n",
    "    # For multi-platform brands, calculate relative performance\n",
    "    brand_platform_performance = df.groupby(['brand_name', 'primary_platform'])['sales_quantity'].mean()\n",
    "    brand_overall_performance = df.groupby('brand_name')['sales_quantity'].mean()\n",
    "    \n",
    "    def calculate_platform_preference_score(row):\n",
    "        if row['is_multi_platform_brand'] == 0:\n",
    "            return 1.0  # Single platform brands get neutral score\n",
    "        \n",
    "        brand = row['brand_name']\n",
    "        platform = row['primary_platform']\n",
    "        \n",
    "        platform_performance = brand_platform_performance.get((brand, platform), 0)\n",
    "        overall_performance = brand_overall_performance.get(brand, 1)\n",
    "        \n",
    "        return platform_performance / overall_performance if overall_performance > 0 else 1.0\n",
    "    \n",
    "    df['brand_platform_preference_score'] = df.apply(calculate_platform_preference_score, axis=1)\n",
    "    \n",
    "    print(\"✓ Created cross-platform competitive dynamics\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_seasonal_interaction_features(df):\n",
    "    \"\"\"\n",
    "    Create advanced seasonal and promotional interaction features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    print(\"\\n=== CREATING SEASONAL INTERACTION FEATURES ===\")\n",
    "    \n",
    "    # Brand-seasonal performance patterns\n",
    "    brand_seasonal_performance = df.groupby(['brand_name', 'month'])['sales_quantity'].mean()\n",
    "    brand_annual_performance = df.groupby('brand_name')['sales_quantity'].mean()\n",
    "    \n",
    "    def get_brand_seasonal_index(row):\n",
    "        brand = row['brand_name']\n",
    "        month = row['month']\n",
    "        \n",
    "        seasonal_perf = brand_seasonal_performance.get((brand, month), 0)\n",
    "        annual_perf = brand_annual_performance.get(brand, 1)\n",
    "        \n",
    "        return seasonal_perf / annual_perf if annual_perf > 0 else 1.0\n",
    "    \n",
    "    df['brand_seasonal_index'] = df.apply(get_brand_seasonal_index, axis=1)\n",
    "    \n",
    "    # Platform-seasonal interaction\n",
    "    platform_seasonal_performance = df.groupby(['primary_platform', 'month'])['sales_quantity'].mean()\n",
    "    platform_annual_performance = df.groupby('primary_platform')['sales_quantity'].mean()\n",
    "    \n",
    "    def get_platform_seasonal_index(row):\n",
    "        platform = row['primary_platform']\n",
    "        month = row['month']\n",
    "        \n",
    "        seasonal_perf = platform_seasonal_performance.get((platform, month), 0)\n",
    "        annual_perf = platform_annual_performance.get(platform, 1)\n",
    "        \n",
    "        return seasonal_perf / annual_perf if annual_perf > 0 else 1.0\n",
    "    \n",
    "    df['platform_seasonal_index'] = df.apply(get_platform_seasonal_index, axis=1)\n",
    "    \n",
    "    # Promotional effectiveness by brand\n",
    "    promo_performance = df[df['is_promotional'] == 1].groupby('brand_name')['sales_quantity'].mean()\n",
    "    non_promo_performance = df[df['is_promotional'] == 0].groupby('brand_name')['sales_quantity'].mean()\n",
    "    \n",
    "    promotional_lift = (promo_performance / non_promo_performance).fillna(1.0)\n",
    "    df['brand_promotional_effectiveness'] = df['brand_name'].map(promotional_lift).fillna(1.0)\n",
    "    \n",
    "    # Complex interactions\n",
    "    df['brand_seasonal_promo_interaction'] = (\n",
    "        df['brand_seasonal_index'] * \n",
    "        df['is_promotional'] * \n",
    "        df['brand_promotional_effectiveness']\n",
    "    )\n",
    "    \n",
    "    df['platform_brand_seasonal_interaction'] = (\n",
    "        df['platform_seasonal_index'] * \n",
    "        df['brand_seasonal_index']\n",
    "    )\n",
    "    \n",
    "    # Trend-season interaction (are trends stronger in certain seasons?)\n",
    "    df['trend_seasonal_interaction'] = (\n",
    "        df['sales_pct_change_3'] * df['monthly_intensity_learned']\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Created seasonal interaction features\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_outlier_and_spike_features(df):\n",
    "    \"\"\"\n",
    "    Create features to help models handle extreme outliers and sales spikes\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    print(\"\\n=== CREATING OUTLIER AND SPIKE DETECTION FEATURES ===\")\n",
    "    \n",
    "    # Historical spike detection for each store-brand combination\n",
    "    spike_threshold_99 = df['sales_quantity'].quantile(0.99)\n",
    "    spike_threshold_95 = df['sales_quantity'].quantile(0.95)\n",
    "    \n",
    "    df['is_extreme_spike'] = (df['sales_quantity'] > spike_threshold_99).astype(int)\n",
    "    df['is_major_spike'] = (df['sales_quantity'] > spike_threshold_95).astype(int)\n",
    "    \n",
    "    # Store-brand spike history\n",
    "    spike_history = df.groupby('store_brand_platform_key').agg({\n",
    "        'is_extreme_spike': 'sum',\n",
    "        'is_major_spike': 'sum'\n",
    "    }).rename(columns={\n",
    "        'is_extreme_spike': 'historical_extreme_spikes',\n",
    "        'is_major_spike': 'historical_major_spikes'\n",
    "    })\n",
    "    \n",
    "    df = df.merge(spike_history, on='store_brand_platform_key', how='left')\n",
    "    \n",
    "    # Spike propensity score\n",
    "    total_records_per_key = df.groupby('store_brand_platform_key').size()\n",
    "    df['spike_propensity'] = df['historical_major_spikes'] / df.groupby('store_brand_platform_key').cumcount().add(1)\n",
    "    \n",
    "    # Distance from normal behavior\n",
    "    df['deviation_from_rolling_mean'] = np.where(\n",
    "        df['sales_rolling_mean_6'] > 0,\n",
    "        (df['sales_quantity'] - df['sales_rolling_mean_6']) / df['sales_rolling_mean_6'],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Z-score based on rolling statistics  \n",
    "    df['rolling_z_score'] = np.where(\n",
    "        df['sales_rolling_std_6'] > 0,\n",
    "        (df['sales_quantity'] - df['sales_rolling_mean_6']) / df['sales_rolling_std_6'],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Created outlier and spike detection features\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Master function to create all advanced features\n",
    "def create_all_advanced_features(df):\n",
    "    \"\"\"\n",
    "    Apply all advanced feature engineering steps\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 3: ADVANCED TEMPORAL DEPENDENCIES & INTERACTIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Apply all feature engineering steps\n",
    "    df = create_lag_and_rolling_features(df)\n",
    "    df = create_cross_platform_dynamics(df)\n",
    "    df = create_seasonal_interaction_features(df)\n",
    "    df = create_outlier_and_spike_features(df)\n",
    "    \n",
    "    # Clean up intermediate columns\n",
    "    cleanup_cols = ['store_brand_platform_key']\n",
    "    df = df.drop(columns=[col for col in cleanup_cols if col in df.columns])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Count features by type\n",
    "    lag_features = [col for col in df.columns if 'lag_' in col]\n",
    "    rolling_features = [col for col in df.columns if 'rolling_' in col]\n",
    "    momentum_features = [col for col in df.columns if any(x in col for x in ['momentum', 'acceleration', 'pct_change', 'volatility'])]\n",
    "    platform_features = [col for col in df.columns if any(x in col for x in ['platform', 'competing', 'multi_platform'])]\n",
    "    seasonal_features = [col for col in df.columns if any(x in col for x in ['seasonal', 'promotional', 'interaction'])]\n",
    "    spike_features = [col for col in df.columns if any(x in col for x in ['spike', 'deviation', 'z_score', 'propensity'])]\n",
    "    \n",
    "    print(f\"Lag Features ({len(lag_features)}): {lag_features}\")\n",
    "    print(f\"Rolling Window Features ({len(rolling_features)}): {rolling_features[:5]}...\" if len(rolling_features) > 5 else f\"Rolling Window Features ({len(rolling_features)}): {rolling_features}\")\n",
    "    print(f\"Momentum Features ({len(momentum_features)}): {momentum_features}\")\n",
    "    print(f\"Platform Dynamics ({len(platform_features)}): {platform_features}\")\n",
    "    print(f\"Seasonal Interactions ({len(seasonal_features)}): {seasonal_features}\")\n",
    "    print(f\"Spike Detection ({len(spike_features)}): {spike_features}\")\n",
    "    \n",
    "    total_engineered_features = len(lag_features) + len(rolling_features) + len(momentum_features) + len(platform_features) + len(seasonal_features) + len(spike_features)\n",
    "    print(f\"\\nTotal Advanced Features Created: {total_engineered_features}\")\n",
    "    print(f\"Final Dataset Shape: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Usage in your notebook:\n",
    "print(\"Ready to create advanced temporal dependencies...\")\n",
    "print(\"Run: df_advanced = create_all_advanced_features(df_with_behavior)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Store Categorization and Model Preparation\n",
    "# Adding store type categorization and preparing for advanced modeling\n",
    "\n",
    "def add_store_categorization(df):\n",
    "    \"\"\"\n",
    "    Add store type categorization based on Chinese e-commerce naming patterns\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"=== ADDING STORE CATEGORIZATION ===\")\n",
    "    \n",
    "    def categorize_store(store_name):\n",
    "        \"\"\"\n",
    "        Categorize stores based on Chinese e-commerce store naming conventions\n",
    "        \"\"\"\n",
    "        if pd.isna(store_name):\n",
    "            return 'Unknown'\n",
    "        \n",
    "        store_name = str(store_name)\n",
    "        \n",
    "        if '官方旗舰店' in store_name:\n",
    "            return 'Official Flagship'\n",
    "        elif '卖场旗舰店' in store_name:\n",
    "            return 'Mall Flagship'\n",
    "        elif '旗舰店' in store_name:\n",
    "            return 'Flagship'\n",
    "        elif '专卖店' in store_name or '专营店' in store_name:\n",
    "            return 'Specialty Store'\n",
    "        elif '卖场店' in store_name:\n",
    "            return 'Mall Store'\n",
    "        elif '自营' in store_name:\n",
    "            return 'Platform Direct'\n",
    "        elif '超市' in store_name:\n",
    "            return 'Supermarket'\n",
    "        else:\n",
    "            return 'Other'\n",
    "    \n",
    "    # Apply categorization\n",
    "    df['store_type'] = df['store_name'].apply(categorize_store)\n",
    "    \n",
    "    # Analyze store type distribution\n",
    "    store_type_analysis = df.groupby(['store_type', 'primary_platform']).agg({\n",
    "        'sales_quantity': ['count', 'sum', 'mean'],\n",
    "        'sales_amount': ['sum', 'mean'],\n",
    "        'unit_price': 'mean',\n",
    "        'store_name': 'nunique'\n",
    "    }).round(2)\n",
    "    \n",
    "    store_type_analysis.columns = ['_'.join(col) for col in store_type_analysis.columns]\n",
    "    print(\"Store Type Analysis by Platform:\")\n",
    "    print(store_type_analysis)\n",
    "    \n",
    "    # Create store type performance features\n",
    "    store_type_performance = df.groupby('store_type').agg({\n",
    "        'sales_quantity': 'mean',\n",
    "        'unit_price': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Store type premium index (compared to average)\n",
    "    avg_sales = df['sales_quantity'].mean()\n",
    "    avg_price = df['unit_price'].mean()\n",
    "    \n",
    "    store_type_performance['sales_performance_index'] = store_type_performance['sales_quantity'] / avg_sales\n",
    "    store_type_performance['price_premium_index'] = store_type_performance['unit_price'] / avg_price\n",
    "    \n",
    "    # Map back to main dataset\n",
    "    df['store_type_sales_index'] = df['store_type'].map(store_type_performance['sales_performance_index']).fillna(1)\n",
    "    df['store_type_price_index'] = df['store_type'].map(store_type_performance['price_premium_index']).fillna(1)\n",
    "    \n",
    "    # One-hot encode store types for modeling\n",
    "    store_type_dummies = pd.get_dummies(df['store_type'], prefix='store_type')\n",
    "    df = pd.concat([df, store_type_dummies], axis=1)\n",
    "    \n",
    "    print(f\"✓ Store categorization complete. Created {len(store_type_dummies.columns)} store type features\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_modeling_dataset(df):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for advanced modeling\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"\\n=== PREPARING DATASET FOR MODELING ===\")\n",
    "    \n",
    "    # Handle any remaining missing values in lag features (expected for early records)\n",
    "    lag_columns = [col for col in df.columns if 'lag_' in col or 'rolling_' in col]\n",
    "    \n",
    "    for col in lag_columns:\n",
    "        if df[col].isnull().any():\n",
    "            # Fill NaN lag values with appropriate defaults\n",
    "            if 'lag_' in col:\n",
    "                # For lag features, use forward fill within groups, then 0\n",
    "                df[col] = df.groupby(['primary_platform', 'store_name', 'brand_name'])[col].ffill().fillna(0)\n",
    "            elif 'rolling_mean' in col:\n",
    "                # For rolling means, use the current value\n",
    "                df[col] = df[col].fillna(df['sales_quantity'])\n",
    "            elif 'rolling_std' in col:\n",
    "                # For rolling std, use 0 (no volatility)\n",
    "                df[col] = df[col].fillna(0)\n",
    "            elif 'rolling_min' in col or 'rolling_max' in col:\n",
    "                # For rolling min/max, use current value\n",
    "                df[col] = df[col].fillna(df['sales_quantity'])\n",
    "    \n",
    "    # Handle any infinite values\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # Create final feature list for modeling\n",
    "    feature_categories = {\n",
    "        'temporal_basic': ['month', 'quarter', 'year', 'days_since_start'],\n",
    "        'temporal_cyclical': [col for col in df.columns if any(x in col for x in ['sin', 'cos'])],\n",
    "        'promotional': [col for col in df.columns if any(x in col for x in ['promotional', 'distance_to_promo'])],\n",
    "        'seasonal': [col for col in df.columns if 'seasonal' in col and 'interaction' not in col],\n",
    "        'lag_features': [col for col in df.columns if 'lag_' in col],\n",
    "        'rolling_features': [col for col in df.columns if 'rolling_' in col],\n",
    "        'momentum': [col for col in df.columns if any(x in col for x in ['momentum', 'acceleration', 'pct_change', 'volatility'])],\n",
    "        'store_behavior': [col for col in df.columns if any(x in col for x in ['store_sales_cv', 'store_sales_range', 'brand_diversity', 'product_diversity'])],\n",
    "        'store_type': [col for col in df.columns if 'store_type_' in col],\n",
    "        'brand_market': [col for col in df.columns if any(x in col for x in ['brand_market_share', 'brand_promotional_effectiveness'])],\n",
    "        'platform_dynamics': [col for col in df.columns if any(x in col for x in ['competing', 'multi_platform', 'platform_preference'])],\n",
    "        'interactions': [col for col in df.columns if 'interaction' in col],\n",
    "        'spike_detection': [col for col in df.columns if any(x in col for x in ['spike', 'deviation', 'z_score', 'propensity'])]\n",
    "    }\n",
    "    \n",
    "    # Print feature summary\n",
    "    total_features = 0\n",
    "    print(\"\\nFEATURE CATEGORIES FOR MODELING:\")\n",
    "    for category, features in feature_categories.items():\n",
    "        if features:\n",
    "            print(f\"{category.upper()}: {len(features)} features\")\n",
    "            total_features += len(features)\n",
    "    \n",
    "    print(f\"\\nTOTAL MODELING FEATURES: {total_features}\")\n",
    "    \n",
    "    # Identify target and exclude non-feature columns\n",
    "    exclude_columns = [\n",
    "        'sales_month', 'store_name', 'brand_name', 'product_code', \n",
    "        'sales_amount', 'sales_quantity', 'unit_price', 'store_type',\n",
    "        'month_year', 'primary_platform', 'secondary_platform'  # Exclude string columns\n",
    "    ]\n",
    "    \n",
    "    modeling_features = [col for col in df.columns if col not in exclude_columns]\n",
    "    \n",
    "    # Ensure we only include numeric features for correlation analysis\n",
    "    numeric_features = df[modeling_features].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    print(f\"FEATURES READY FOR MODELING: {len(modeling_features)}\")\n",
    "    print(f\"NUMERIC FEATURES FOR CORRELATION: {len(numeric_features)}\")\n",
    "    print(f\"TARGET VARIABLE: sales_quantity\")\n",
    "    \n",
    "    # Data quality check\n",
    "    print(f\"\\nDATA QUALITY CHECK:\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Missing values in features: {df[modeling_features].isnull().sum().sum()}\")\n",
    "    print(f\"Infinite values in numeric features: {np.isinf(df[numeric_features]).sum().sum()}\")\n",
    "    \n",
    "    # Feature correlation analysis (top correlated features with target) - only numeric features\n",
    "    target_correlations = df[numeric_features + ['sales_quantity']].corr()['sales_quantity'].abs().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nTOP 10 FEATURES CORRELATED WITH TARGET:\")\n",
    "    for i, (feature, corr) in enumerate(target_correlations.head(11).items()):\n",
    "        if feature != 'sales_quantity':  # Skip target itself\n",
    "            print(f\"{i+1:2d}. {feature:<40} {corr:.4f}\")\n",
    "    \n",
    "    return df, modeling_features\n",
    "\n",
    "def create_rolling_time_series_splits(df):\n",
    "    \"\"\"\n",
    "    Create rolling time series splits for comprehensive seasonal validation\n",
    "    Option 2: Multiple train/val splits across different seasons\n",
    "    \n",
    "    This approach tests model performance across ALL seasons, not just one quarter,\n",
    "    providing much more robust validation of seasonal pattern learning.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== CREATING ROLLING TIME SERIES SPLITS ===\")\n",
    "    \n",
    "    # Sort by date to ensure proper time series order\n",
    "    df = df.sort_values('sales_month')\n",
    "    \n",
    "    print(f\"Total records: {len(df):,}\")\n",
    "    print(f\"Date range: {df['sales_month'].min()} to {df['sales_month'].max()}\")\n",
    "    \n",
    "    # Create 4 rolling splits to test across all seasons\n",
    "    splits = []\n",
    "    \n",
    "    # Split 1: Train on 2021 full year, Validate on 2022 Q1 (Jan-Mar)\n",
    "    train_1 = df[df['sales_month'].dt.year == 2021].copy()\n",
    "    val_1 = df[(df['sales_month'].dt.year == 2022) & \n",
    "               (df['sales_month'].dt.month.isin([1,2,3]))].copy()\n",
    "    splits.append((train_1, val_1, \"2021_full → 2022_Q1\"))\n",
    "    \n",
    "    # Split 2: Train on 2021 + 2022 Q1, Validate on 2022 Q2 (Apr-Jun)\n",
    "    train_2 = df[df['sales_month'] <= '2022-03-01'].copy()\n",
    "    val_2 = df[(df['sales_month'].dt.year == 2022) & \n",
    "               (df['sales_month'].dt.month.isin([4,5,6]))].copy()\n",
    "    splits.append((train_2, val_2, \"2021+2022Q1 → 2022_Q2\"))\n",
    "    \n",
    "    # Split 3: Train on 2021 + 2022 H1, Validate on 2022 Q3 (Jul-Sep)  \n",
    "    train_3 = df[df['sales_month'] <= '2022-06-01'].copy()\n",
    "    val_3 = df[(df['sales_month'].dt.year == 2022) & \n",
    "               (df['sales_month'].dt.month.isin([7,8,9]))].copy()\n",
    "    splits.append((train_3, val_3, \"2021+2022H1 → 2022_Q3\"))\n",
    "    \n",
    "    # Split 4: Train on 2021 + 2022 Q1-Q3, Validate on 2022 Q4 (Oct-Dec)\n",
    "    train_4 = df[df['sales_month'] <= '2022-09-01'].copy()\n",
    "    val_4 = df[(df['sales_month'].dt.year == 2022) & \n",
    "               (df['sales_month'].dt.month.isin([10,11,12]))].copy()\n",
    "    splits.append((train_4, val_4, \"2021+2022Q1Q2Q3 → 2022_Q4\"))\n",
    "    \n",
    "    # Print split information\n",
    "    print(f\"\\nROLLING TIME SERIES SPLITS CREATED:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, (train, val, description) in enumerate(splits):\n",
    "        print(f\"SPLIT {i+1}: {description}\")\n",
    "        print(f\"  Train: {len(train):,} records ({train['sales_month'].min()} to {train['sales_month'].max()})\")\n",
    "        print(f\"  Val:   {len(val):,} records ({val['sales_month'].min()} to {val['sales_month'].max()})\")\n",
    "        \n",
    "        # Show seasonal coverage\n",
    "        train_months = sorted(train['sales_month'].dt.month.unique())\n",
    "        val_months = sorted(val['sales_month'].dt.month.unique())\n",
    "        print(f\"  Train months: {train_months}\")\n",
    "        print(f\"  Val months: {val_months}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"BENEFITS OF THIS APPROACH:\")\n",
    "    print(\"✓ Tests model performance across ALL four seasons\")\n",
    "    print(\"✓ Validates seasonal pattern learning comprehensively\") \n",
    "    print(\"✓ Handles Douyin data gap (2021 Apr-Dec) elegantly\")\n",
    "    print(\"✓ Enables robust model selection and hyperparameter tuning\")\n",
    "    print(\"✓ Identifies seasonal biases in model performance\")\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# Master function\n",
    "def complete_feature_engineering_and_prep(df):\n",
    "    \"\"\"\n",
    "    Complete the feature engineering process and prepare for modeling\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"STEP 4: STORE CATEGORIZATION & MODEL PREPARATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Add store categorization\n",
    "    df = add_store_categorization(df)\n",
    "    \n",
    "    # Prepare for modeling\n",
    "    df, modeling_features = prepare_modeling_dataset(df)\n",
    "    \n",
    "    # Create rolling time series splits\n",
    "    rolling_splits = create_rolling_time_series_splits(df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"READY FOR ADVANCED MODELING!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"✓ {len(modeling_features)} features engineered\")\n",
    "    print(f\"✓ Store categorization applied\") \n",
    "    print(f\"✓ Rolling time series splits created ({len(rolling_splits)} splits)\")\n",
    "    print(f\"✓ Data quality validated\")\n",
    "    print(\"\\nNext: Advanced deep learning models to break 20% MAPE barrier!\")\n",
    "    \n",
    "    return df, modeling_features, rolling_splits\n",
    "\n",
    "# Usage in your notebook:\n",
    "print(\"Ready to complete feature engineering and model preparation...\")\n",
    "print(\"Run: df_final, features, rolling_splits = complete_feature_engineering_and_prep(df_advanced)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the engineered dataset with all features\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "def save_engineered_dataset(df_final, features, rolling_splits):\n",
    "    \"\"\"\n",
    "    Save the complete engineered dataset and rolling splits for future use\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    print(\"=== SAVING ENGINEERED DATASET ===\")\n",
    "    \n",
    "    # Save main dataset with all features\n",
    "    main_filename = f\"sales_forecast_engineered_dataset_{timestamp}.pkl\"\n",
    "    \n",
    "    # Create a comprehensive save package\n",
    "    dataset_package = {\n",
    "        'df_final': df_final,\n",
    "        'modeling_features': features,\n",
    "        'rolling_splits': rolling_splits,\n",
    "        'metadata': {\n",
    "            'total_records': len(df_final),\n",
    "            'total_features': len(features),\n",
    "            'date_range': {\n",
    "                'start': df_final['sales_month'].min(),\n",
    "                'end': df_final['sales_month'].max()\n",
    "            },\n",
    "            'platforms': df_final['primary_platform'].unique().tolist(),\n",
    "            'rolling_splits_info': [\n",
    "                {\n",
    "                    'split_num': i+1,\n",
    "                    'description': description,\n",
    "                    'train_records': len(train),\n",
    "                    'val_records': len(val),\n",
    "                    'train_period': {\n",
    "                        'start': train['sales_month'].min(),\n",
    "                        'end': train['sales_month'].max()\n",
    "                    },\n",
    "                    'val_period': {\n",
    "                        'start': val['sales_month'].min(),\n",
    "                        'end': val['sales_month'].max()\n",
    "                    }\n",
    "                }\n",
    "                for i, (train, val, description) in enumerate(rolling_splits)\n",
    "            ],\n",
    "            'feature_categories': {\n",
    "                'temporal_basic': [col for col in features if any(x in col for x in ['month', 'quarter', 'year', 'days_since'])],\n",
    "                'temporal_cyclical': [col for col in features if any(x in col for x in ['sin', 'cos'])],\n",
    "                'promotional': [col for col in features if any(x in col for x in ['promotional', 'distance_to_promo'])],\n",
    "                'seasonal': [col for col in features if 'seasonal' in col and 'interaction' not in col],\n",
    "                'lag_features': [col for col in features if 'lag_' in col],\n",
    "                'rolling_features': [col for col in features if 'rolling_' in col],\n",
    "                'momentum': [col for col in features if any(x in col for x in ['momentum', 'acceleration', 'pct_change', 'volatility'])],\n",
    "                'store_behavior': [col for col in features if any(x in col for x in ['store_sales_cv', 'store_sales_range', 'brand_diversity', 'product_diversity'])],\n",
    "                'store_type': [col for col in features if 'store_type_' in col],\n",
    "                'brand_market': [col for col in features if any(x in col for x in ['brand_market_share', 'brand_promotional_effectiveness'])],\n",
    "                'platform_dynamics': [col for col in features if any(x in col for x in ['competing', 'multi_platform', 'platform_preference'])],\n",
    "                'interactions': [col for col in features if 'interaction' in col],\n",
    "                'spike_detection': [col for col in features if any(x in col for x in ['spike', 'deviation', 'z_score', 'propensity'])]\n",
    "            },\n",
    "            'creation_timestamp': timestamp\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save as pickle for preserving data types\n",
    "    with open(main_filename, 'wb') as f:\n",
    "        pickle.dump(dataset_package, f)\n",
    "    \n",
    "    print(f\"✓ Complete dataset saved as: {main_filename}\")\n",
    "    \n",
    "    # Also save a CSV version of the main dataset (without splits) for external tools\n",
    "    csv_filename = f\"sales_forecast_engineered_dataset_{timestamp}.csv\"\n",
    "    df_final.to_csv(csv_filename, index=False)\n",
    "    print(f\"✓ CSV version saved as: {csv_filename}\")\n",
    "    \n",
    "    # Save feature list as text file for reference\n",
    "    features_filename = f\"modeling_features_{timestamp}.txt\"\n",
    "    with open(features_filename, 'w') as f:\n",
    "        f.write(\"MODELING FEATURES FOR SALES FORECASTING\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\\n\")\n",
    "        \n",
    "        for category, feature_list in dataset_package['metadata']['feature_categories'].items():\n",
    "            if feature_list:\n",
    "                f.write(f\"{category.upper()} ({len(feature_list)} features):\\n\")\n",
    "                for feature in feature_list:\n",
    "                    f.write(f\"  - {feature}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "        \n",
    "        f.write(f\"TOTAL FEATURES: {len(features)}\\n\")\n",
    "        f.write(f\"DATASET CREATED: {timestamp}\\n\")\n",
    "    \n",
    "    print(f\"✓ Feature documentation saved as: {features_filename}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nDATASET SAVE SUMMARY:\")\n",
    "    print(f\"Total Records: {len(df_final):,}\")\n",
    "    print(f\"Total Features: {len(features)}\")\n",
    "    print(f\"Rolling Splits: {len(rolling_splits)}\")\n",
    "    for i, (train, val, description) in enumerate(rolling_splits):\n",
    "        print(f\"  Split {i+1}: {len(train):,} train, {len(val):,} val ({description})\")\n",
    "    print(f\"Date Range: {df_final['sales_month'].min()} to {df_final['sales_month'].max()}\")\n",
    "    \n",
    "    return main_filename, csv_filename, features_filename\n",
    "\n",
    "def load_engineered_dataset(pickle_filename):\n",
    "    \"\"\"\n",
    "    Load the saved engineered dataset\n",
    "    \"\"\"\n",
    "    print(f\"Loading engineered dataset from: {pickle_filename}\")\n",
    "    \n",
    "    with open(pickle_filename, 'rb') as f:\n",
    "        dataset_package = pickle.load(f)\n",
    "    \n",
    "    print(\"✓ Dataset loaded successfully\")\n",
    "    print(f\"Records: {len(dataset_package['df_final']):,}\")\n",
    "    print(f\"Features: {len(dataset_package['modeling_features'])}\")\n",
    "    print(f\"Rolling Splits: {len(dataset_package['rolling_splits'])}\")\n",
    "    print(f\"Created: {dataset_package['metadata']['creation_timestamp']}\")\n",
    "    \n",
    "    return (\n",
    "        dataset_package['df_final'],\n",
    "        dataset_package['modeling_features'],\n",
    "        dataset_package['rolling_splits'],\n",
    "        dataset_package['metadata']\n",
    "    )\n",
    "\n",
    "# Quick feature summary function\n",
    "def print_feature_summary(features):\n",
    "    \"\"\"\n",
    "    Print a quick summary of feature categories\n",
    "    \"\"\"\n",
    "    feature_categories = {\n",
    "        'Temporal': [col for col in features if any(x in col for x in ['month', 'quarter', 'year', 'sin', 'cos', 'days_since'])],\n",
    "        'Promotional': [col for col in features if any(x in col for x in ['promotional', 'distance_to_promo'])],\n",
    "        'Seasonal': [col for col in features if 'seasonal' in col],\n",
    "        'Lag': [col for col in features if 'lag_' in col],\n",
    "        'Rolling': [col for col in features if 'rolling_' in col],\n",
    "        'Momentum': [col for col in features if any(x in col for x in ['momentum', 'acceleration', 'pct_change', 'volatility'])],\n",
    "        'Store': [col for col in features if any(x in col for x in ['store_', 'brand_diversity', 'product_diversity'])],\n",
    "        'Platform': [col for col in features if any(x in col for x in ['competing', 'multi_platform', 'platform_preference'])],\n",
    "        'Interactions': [col for col in features if 'interaction' in col],\n",
    "        'Spikes': [col for col in features if any(x in col for x in ['spike', 'deviation', 'z_score', 'propensity'])]\n",
    "    }\n",
    "    \n",
    "    print(\"FEATURE SUMMARY:\")\n",
    "    total = 0\n",
    "    for category, feature_list in feature_categories.items():\n",
    "        print(f\"{category:15s}: {len(feature_list):2d} features\")\n",
    "        total += len(feature_list)\n",
    "    print(f\"{'TOTAL':15s}: {total:2d} features\")\n",
    "\n",
    "# Usage example\n",
    "print(\"Ready to save the engineered dataset!\")\n",
    "print(\"Run: pickle_file, csv_file, features_file = save_engineered_dataset(df_final, features, rolling_splits)\")\n",
    "print(\"\\nTo load later: df, features, rolling_splits, metadata = load_engineered_dataset('filename.pkl')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mingyin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
