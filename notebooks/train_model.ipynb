{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final, features, rolling_splits, metadata = load_engineered_dataset('sales_forecast_engineered_dataset_20250528_170657.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Advanced Embedding-Based Deep Learning (Complete)\n",
    "# No nested methods, clean indentation, with prediction saving functionality\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model, callbacks\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define custom metrics as standalone functions\n",
    "def mape_metric_original_scale(y_true, y_pred):\n",
    "    \"\"\"MAPE metric in original scale for monitoring during training\"\"\"\n",
    "    y_true_orig = tf.exp(y_true) - 1\n",
    "    y_pred_orig = tf.exp(y_pred) - 1\n",
    "    y_true_orig = tf.clip_by_value(y_true_orig, 1.0, 1e6)\n",
    "    y_pred_orig = tf.clip_by_value(y_pred_orig, 1.0, 1e6)\n",
    "    epsilon = 1.0\n",
    "    mape = tf.reduce_mean(tf.abs(y_true_orig - y_pred_orig) / (y_true_orig + epsilon)) * 100\n",
    "    return tf.clip_by_value(mape, 0.0, 1000.0)\n",
    "\n",
    "def rmse_metric_original_scale(y_true, y_pred):\n",
    "    \"\"\"RMSE in original scale for monitoring during training\"\"\"\n",
    "    y_true_orig = tf.exp(y_true) - 1\n",
    "    y_pred_orig = tf.exp(y_pred) - 1\n",
    "    y_true_orig = tf.clip_by_value(y_true_orig, 1.0, 1e6)\n",
    "    y_pred_orig = tf.clip_by_value(y_pred_orig, 1.0, 1e6)\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true_orig - y_pred_orig)))\n",
    "\n",
    "class AdvancedEmbeddingModel:\n",
    "    \"\"\"\n",
    "    Advanced embedding-based deep learning for sales forecasting\n",
    "    Clean structure without nested methods\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_seed=42):\n",
    "        self.random_seed = random_seed\n",
    "        self.encoders = {}\n",
    "        self.scalers = {}\n",
    "        tf.random.set_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        print(\"=\" * 70)\n",
    "        print(\"ADVANCED EMBEDDING-BASED FRAMEWORK INITIALIZED\")\n",
    "        print(\"=\" * 70)\n",
    "    \n",
    "    def safe_mape_calculation(self, y_true, y_pred):\n",
    "        \"\"\"Safe MAPE calculation with proper error handling\"\"\"\n",
    "        y_true_orig = np.expm1(y_true)\n",
    "        y_pred_orig = np.expm1(y_pred)\n",
    "        y_pred_orig = np.clip(y_pred_orig, 0.1, 1e6)\n",
    "        y_true_orig = np.clip(y_true_orig, 0.1, 1e6)\n",
    "        epsilon = 1.0\n",
    "        ape = np.abs(y_true_orig - y_pred_orig) / (y_true_orig + epsilon)\n",
    "        mape = np.mean(ape) * 100\n",
    "        return min(mape, 1000.0)\n",
    "    \n",
    "    def save_detailed_predictions(self, val_data, val_pred_orig, val_true_orig, split_num, description, model_type, timestamp):\n",
    "        \"\"\"Save detailed predictions with comprehensive analysis\"\"\"\n",
    "        # Create results DataFrame\n",
    "        results_df = val_data.copy()\n",
    "        results_df['predicted_sales'] = val_pred_orig\n",
    "        results_df['actual_sales'] = val_true_orig\n",
    "        results_df['absolute_error'] = np.abs(val_pred_orig - val_true_orig)\n",
    "        results_df['absolute_percentage_error'] = np.abs(val_pred_orig - val_true_orig) / (val_true_orig + 1) * 100\n",
    "        results_df['is_perfect_prediction'] = results_df['absolute_error'] < 1\n",
    "        \n",
    "        # Add error categories\n",
    "        def categorize_error(ape):\n",
    "            if ape < 5: return \"Excellent (<5%)\"\n",
    "            elif ape < 10: return \"Very Good (5-10%)\"\n",
    "            elif ape < 20: return \"Good (10-20%)\"\n",
    "            elif ape < 50: return \"Fair (20-50%)\"\n",
    "            else: return \"Poor (>50%)\"\n",
    "        \n",
    "        results_df['error_category'] = results_df['absolute_percentage_error'].apply(categorize_error)\n",
    "        \n",
    "        # Save detailed predictions\n",
    "        predictions_filename = f\"detailed_predictions_split_{split_num}_{model_type}_{timestamp}.csv\"\n",
    "        results_df.to_csv(predictions_filename, index=False)\n",
    "        \n",
    "        # Save summary predictions (top-level metrics only)\n",
    "        summary_cols = ['sales_month', 'primary_platform', 'store_name', 'brand_name', \n",
    "                       'actual_sales', 'predicted_sales', 'absolute_percentage_error', 'error_category']\n",
    "        available_cols = [col for col in summary_cols if col in results_df.columns]\n",
    "        \n",
    "        summary_filename = f\"summary_predictions_split_{split_num}_{model_type}_{timestamp}.csv\"\n",
    "        results_df[available_cols].to_csv(summary_filename, index=False)\n",
    "        \n",
    "        print(f\"  📊 Predictions saved:\")\n",
    "        print(f\"    Detailed: {predictions_filename}\")\n",
    "        print(f\"    Summary: {summary_filename}\")\n",
    "        \n",
    "        return results_df, {'detailed_predictions': predictions_filename, 'summary_predictions': summary_filename}\n",
    "    \n",
    "    def save_split_analysis_report(self, results_df, split_num, description, model_type, timestamp):\n",
    "        \"\"\"Save comprehensive analysis report for a training split\"\"\"\n",
    "        filename = f\"training_analysis_report_split_{split_num}_{model_type}_{timestamp}.txt\"\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(f\"TRAINING SPLIT ANALYSIS REPORT\\n\")\n",
    "            f.write(f\"=\" * 50 + \"\\n\")\n",
    "            f.write(f\"Split: {split_num} - {description}\\n\")\n",
    "            f.write(f\"Model Type: {model_type}\\n\")\n",
    "            f.write(f\"Generated: {pd.Timestamp.now()}\\n\")\n",
    "            f.write(f\"Total Predictions: {len(results_df):,}\\n\\n\")\n",
    "            \n",
    "            # Overall metrics\n",
    "            f.write(f\"OVERALL PERFORMANCE METRICS\\n\")\n",
    "            f.write(f\"-\" * 30 + \"\\n\")\n",
    "            f.write(f\"Mean Absolute Percentage Error: {results_df['absolute_percentage_error'].mean():.2f}%\\n\")\n",
    "            f.write(f\"Median Absolute Percentage Error: {results_df['absolute_percentage_error'].median():.2f}%\\n\")\n",
    "            f.write(f\"Standard Deviation of APE: {results_df['absolute_percentage_error'].std():.2f}%\\n\")\n",
    "            f.write(f\"Mean Absolute Error: {results_df['absolute_error'].mean():.0f} units\\n\")\n",
    "            f.write(f\"Root Mean Square Error: {np.sqrt(np.mean(results_df['absolute_error']**2)):.0f} units\\n\\n\")\n",
    "            \n",
    "            # Error distribution\n",
    "            f.write(f\"ERROR DISTRIBUTION BY CATEGORY\\n\")\n",
    "            f.write(f\"-\" * 30 + \"\\n\")\n",
    "            error_dist = results_df['error_category'].value_counts()\n",
    "            for category in error_dist.index:\n",
    "                count = error_dist[category]\n",
    "                percentage = count / len(results_df) * 100\n",
    "                f.write(f\"{category}: {count:,} ({percentage:.1f}%)\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Percentile analysis\n",
    "            f.write(f\"ERROR PERCENTILES\\n\")\n",
    "            f.write(f\"-\" * 18 + \"\\n\")\n",
    "            percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "            ape_percentiles = np.percentile(results_df['absolute_percentage_error'], percentiles)\n",
    "            for p, value in zip(percentiles, ape_percentiles):\n",
    "                f.write(f\"{p:2d}th percentile: {value:.2f}%\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Platform analysis (if available)\n",
    "            if 'primary_platform' in results_df.columns:\n",
    "                f.write(f\"PERFORMANCE BY PLATFORM\\n\")\n",
    "                f.write(f\"-\" * 25 + \"\\n\")\n",
    "                platform_stats = results_df.groupby('primary_platform').agg({\n",
    "                    'absolute_percentage_error': ['mean', 'median', 'std', 'count'],\n",
    "                    'is_perfect_prediction': 'sum'\n",
    "                }).round(2)\n",
    "                \n",
    "                for platform in platform_stats.index:\n",
    "                    f.write(f\"{platform}:\\n\")\n",
    "                    f.write(f\"  Mean APE: {platform_stats.loc[platform, ('absolute_percentage_error', 'mean')]:.2f}%\\n\")\n",
    "                    f.write(f\"  Median APE: {platform_stats.loc[platform, ('absolute_percentage_error', 'median')]:.2f}%\\n\")\n",
    "                    f.write(f\"  Samples: {platform_stats.loc[platform, ('absolute_percentage_error', 'count')]:,}\\n\")\n",
    "                    f.write(f\"  Perfect predictions: {platform_stats.loc[platform, ('is_perfect_prediction', 'sum')]:,}\\n\\n\")\n",
    "            \n",
    "            # Time-based analysis (if available)\n",
    "            if 'sales_month' in results_df.columns:\n",
    "                f.write(f\"PERFORMANCE BY MONTH\\n\")\n",
    "                f.write(f\"-\" * 20 + \"\\n\")\n",
    "                monthly_stats = results_df.groupby(results_df['sales_month'].dt.month).agg({\n",
    "                    'absolute_percentage_error': ['mean', 'count'],\n",
    "                    'is_perfect_prediction': 'sum'\n",
    "                }).round(2)\n",
    "                \n",
    "                for month in sorted(monthly_stats.index):\n",
    "                    f.write(f\"Month {month:2d}: \")\n",
    "                    f.write(f\"{monthly_stats.loc[month, ('absolute_percentage_error', 'mean')]:.2f}% MAPE \")\n",
    "                    f.write(f\"({monthly_stats.loc[month, ('absolute_percentage_error', 'count')]:,} samples)\\n\")\n",
    "            \n",
    "            # Brand analysis (if available)\n",
    "            if 'brand_name' in results_df.columns:\n",
    "                f.write(f\"\\nTOP 10 BRANDS BY SAMPLE COUNT\\n\")\n",
    "                f.write(f\"-\" * 30 + \"\\n\")\n",
    "                brand_stats = results_df.groupby('brand_name').agg({\n",
    "                    'absolute_percentage_error': ['mean', 'count'],\n",
    "                    'is_perfect_prediction': 'sum'\n",
    "                }).round(2)\n",
    "                \n",
    "                top_brands = brand_stats.nlargest(10, ('absolute_percentage_error', 'count'))\n",
    "                for brand in top_brands.index:\n",
    "                    mean_ape = top_brands.loc[brand, ('absolute_percentage_error', 'mean')]\n",
    "                    count = top_brands.loc[brand, ('absolute_percentage_error', 'count')]\n",
    "                    perfect = top_brands.loc[brand, ('is_perfect_prediction', 'sum')]\n",
    "                    f.write(f\"{brand}: {mean_ape:.2f}% MAPE ({count:,} samples, {perfect:,} perfect)\\n\")\n",
    "            \n",
    "            # Store analysis (if available)  \n",
    "            if 'store_name' in results_df.columns:\n",
    "                f.write(f\"\\nTOP 10 STORES BY SAMPLE COUNT\\n\")\n",
    "                f.write(f\"-\" * 30 + \"\\n\")\n",
    "                store_stats = results_df.groupby('store_name').agg({\n",
    "                    'absolute_percentage_error': ['mean', 'count'],\n",
    "                    'is_perfect_prediction': 'sum'\n",
    "                }).round(2)\n",
    "                \n",
    "                top_stores = store_stats.nlargest(10, ('absolute_percentage_error', 'count'))\n",
    "                for store in top_stores.index:\n",
    "                    mean_ape = top_stores.loc[store, ('absolute_percentage_error', 'mean')]\n",
    "                    count = top_stores.loc[store, ('absolute_percentage_error', 'count')]\n",
    "                    perfect = top_stores.loc[store, ('is_perfect_prediction', 'sum')]\n",
    "                    f.write(f\"{store}: {mean_ape:.2f}% MAPE ({count:,} samples, {perfect:,} perfect)\\n\")\n",
    "            \n",
    "            # Suspicious patterns\n",
    "            f.write(f\"\\nSUSPICION ANALYSIS\\n\")\n",
    "            f.write(f\"-\" * 18 + \"\\n\")\n",
    "            perfect_count = results_df['is_perfect_prediction'].sum()\n",
    "            perfect_pct = perfect_count / len(results_df) * 100\n",
    "            f.write(f\"Perfect predictions (<1 unit error): {perfect_count:,} ({perfect_pct:.1f}%)\\n\")\n",
    "            \n",
    "            if perfect_pct > 5:\n",
    "                f.write(f\"⚠️ WARNING: High percentage of perfect predictions may indicate data leakage\\n\")\n",
    "            \n",
    "            mean_ape = results_df['absolute_percentage_error'].mean()\n",
    "            if mean_ape < 5:\n",
    "                f.write(f\"⚠️ WARNING: Very low MAPE ({mean_ape:.2f}%) may indicate technical issues\\n\")\n",
    "            \n",
    "            # Prediction range analysis\n",
    "            pred_min = results_df['predicted_sales'].min()\n",
    "            pred_max = results_df['predicted_sales'].max()\n",
    "            actual_min = results_df['actual_sales'].min()\n",
    "            actual_max = results_df['actual_sales'].max()\n",
    "            \n",
    "            f.write(f\"\\nPREDICTION RANGE ANALYSIS\\n\")\n",
    "            f.write(f\"-\" * 25 + \"\\n\")\n",
    "            f.write(f\"Predicted sales range: [{pred_min:.0f}, {pred_max:.0f}]\\n\")\n",
    "            f.write(f\"Actual sales range: [{actual_min:.0f}, {actual_max:.0f}]\\n\")\n",
    "            f.write(f\"Range coverage ratio: {(pred_max - pred_min) / (actual_max - actual_min):.2f}\\n\")\n",
    "            \n",
    "            # Worst predictions\n",
    "            f.write(f\"\\nWORST PREDICTIONS (Top 10)\\n\")\n",
    "            f.write(f\"-\" * 27 + \"\\n\")\n",
    "            worst_predictions = results_df.nlargest(10, 'absolute_percentage_error')\n",
    "            for idx, row in worst_predictions.iterrows():\n",
    "                f.write(f\"Actual: {row['actual_sales']:8.0f}, Predicted: {row['predicted_sales']:8.0f}, \")\n",
    "                f.write(f\"APE: {row['absolute_percentage_error']:6.1f}%\")\n",
    "                if 'store_name' in row and 'brand_name' in row:\n",
    "                    f.write(f\" ({row['store_name']}, {row['brand_name']})\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Best predictions\n",
    "            f.write(f\"\\nBEST PREDICTIONS (Top 10)\\n\")\n",
    "            f.write(f\"-\" * 26 + \"\\n\")\n",
    "            best_predictions = results_df.nsmallest(10, 'absolute_percentage_error')\n",
    "            for idx, row in best_predictions.iterrows():\n",
    "                f.write(f\"Actual: {row['actual_sales']:8.0f}, Predicted: {row['predicted_sales']:8.0f}, \")\n",
    "                f.write(f\"APE: {row['absolute_percentage_error']:6.1f}%\")\n",
    "                if 'store_name' in row and 'brand_name' in row:\n",
    "                    f.write(f\" ({row['store_name']}, {row['brand_name']})\")\n",
    "                f.write(\"\\n\")\n",
    "        \n",
    "        print(f\"  📊 Analysis report saved: {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    def categorize_features_for_embeddings(self, df, features):\n",
    "        \"\"\"Analyze and categorize features for embedding strategies\"\"\"\n",
    "        print(\"=== ANALYZING FEATURES FOR EMBEDDING STRATEGIES ===\")\n",
    "        \n",
    "        feature_categories = {\n",
    "            'temporal': [],\n",
    "            'numerical_continuous': [],\n",
    "            'numerical_discrete': [],\n",
    "            'binary': [],\n",
    "            'interactions': []\n",
    "        }\n",
    "        \n",
    "        for feature in features:\n",
    "            if feature not in df.columns:\n",
    "                continue\n",
    "            \n",
    "            dtype = df[feature].dtype\n",
    "            unique_count = df[feature].nunique()\n",
    "            \n",
    "            if any(x in feature for x in ['month', 'quarter', 'day']):\n",
    "                if 'sin' in feature or 'cos' in feature:\n",
    "                    feature_categories['numerical_discrete'].append(feature)\n",
    "                else:\n",
    "                    feature_categories['temporal'].append(feature)\n",
    "            elif any(x in feature for x in ['lag_', 'rolling_', 'sales_', 'momentum', 'volatility']):\n",
    "                feature_categories['numerical_continuous'].append(feature)\n",
    "            elif 'store_type_' in feature or dtype == 'bool':\n",
    "                feature_categories['binary'].append(feature)\n",
    "            elif 'interaction' in feature:\n",
    "                feature_categories['interactions'].append(feature)\n",
    "            else:\n",
    "                if dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "                    if unique_count < 20:\n",
    "                        feature_categories['numerical_discrete'].append(feature)\n",
    "                    else:\n",
    "                        feature_categories['numerical_continuous'].append(feature)\n",
    "        \n",
    "        print(\"Feature categories:\")\n",
    "        for category, feat_list in feature_categories.items():\n",
    "            if feat_list:\n",
    "                print(f\"  {category}: {len(feat_list)} features\")\n",
    "        \n",
    "        return feature_categories\n",
    "    \n",
    "    def prepare_embedding_features(self, df, feature_categories, is_training=True):\n",
    "        \"\"\"Prepare features for embedding-based model\"\"\"\n",
    "        df_work = df.copy()\n",
    "        \n",
    "        if 'sales_quantity_log' not in df_work.columns:\n",
    "            df_work['sales_quantity_log'] = np.log1p(df_work['sales_quantity'])\n",
    "        \n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Temporal features - create embeddings\n",
    "        temporal_features = feature_categories['temporal']\n",
    "        if temporal_features:\n",
    "            temporal_data = []\n",
    "            for feature in temporal_features:\n",
    "                if feature in df_work.columns:\n",
    "                    values = df_work[feature].fillna(0).values.astype(int)\n",
    "                    if feature == 'month':\n",
    "                        values = np.clip(values, 1, 12) - 1  # 0-11 for embedding\n",
    "                    elif feature == 'quarter':\n",
    "                        values = np.clip(values, 1, 4) - 1   # 0-3 for embedding\n",
    "                    else:\n",
    "                        values = np.clip(values, 0, 100)     # General clipping\n",
    "                    temporal_data.append(values)\n",
    "            \n",
    "            if temporal_data:\n",
    "                prepared_data['temporal'] = np.column_stack(temporal_data)\n",
    "        \n",
    "        # Numerical continuous - bucketize and embed\n",
    "        continuous_features = feature_categories['numerical_continuous']\n",
    "        if continuous_features:\n",
    "            continuous_data = []\n",
    "            for feature in continuous_features:\n",
    "                if feature in df_work.columns:\n",
    "                    values = df_work[feature].replace([np.inf, -np.inf], np.nan).fillna(0).values\n",
    "                    \n",
    "                    if is_training:\n",
    "                        # Create quantile-based buckets\n",
    "                        try:\n",
    "                            buckets = np.quantile(values[values != 0], np.linspace(0, 1, 51))  # 50 buckets\n",
    "                            buckets = np.unique(buckets)\n",
    "                            self.encoders[f'{feature}_buckets'] = buckets\n",
    "                        except:\n",
    "                            self.encoders[f'{feature}_buckets'] = np.array([0, 1])\n",
    "                    \n",
    "                    bucket_edges = self.encoders.get(f'{feature}_buckets', np.array([0, 1]))\n",
    "                    bucket_indices = np.digitize(values, bucket_edges)\n",
    "                    bucket_indices = np.clip(bucket_indices, 0, len(bucket_edges))\n",
    "                    continuous_data.append(bucket_indices)\n",
    "            \n",
    "            if continuous_data:\n",
    "                prepared_data['continuous'] = np.column_stack(continuous_data)\n",
    "        \n",
    "        # Direct numerical features\n",
    "        direct_features = (feature_categories['numerical_discrete'] + \n",
    "                          feature_categories['binary'] + \n",
    "                          feature_categories['interactions'])\n",
    "        \n",
    "        if direct_features:\n",
    "            existing_features = [f for f in direct_features if f in df_work.columns]\n",
    "            if existing_features:\n",
    "                direct_data = df_work[existing_features].values.astype(np.float32)\n",
    "                direct_data = np.nan_to_num(direct_data, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "                \n",
    "                if is_training:\n",
    "                    self.scalers['direct'] = RobustScaler()\n",
    "                    direct_data = self.scalers['direct'].fit_transform(direct_data)\n",
    "                else:\n",
    "                    direct_data = self.scalers['direct'].transform(direct_data)\n",
    "                \n",
    "                prepared_data['direct'] = direct_data\n",
    "        \n",
    "        # Target\n",
    "        target = df_work['sales_quantity_log'].values.astype(np.float32)\n",
    "        target = np.nan_to_num(target, nan=0.0, posinf=10.0, neginf=-1.0)\n",
    "        \n",
    "        return prepared_data, target\n",
    "    \n",
    "    def create_advanced_embedding_model(self, feature_categories, data_shapes):\n",
    "        \"\"\"Create advanced embedding-based neural network\"\"\"\n",
    "        print(\"\\n=== CREATING ADVANCED EMBEDDING MODEL ===\")\n",
    "        \n",
    "        inputs = {}\n",
    "        embedding_outputs = []\n",
    "        total_embedding_dim = 0\n",
    "        \n",
    "        # Temporal embeddings\n",
    "        if 'temporal' in data_shapes:\n",
    "            temporal_input = layers.Input(shape=(data_shapes['temporal'],), name='temporal_input')\n",
    "            inputs['temporal'] = temporal_input\n",
    "            \n",
    "            # Process each temporal feature with specific embeddings\n",
    "            temporal_embeddings = []\n",
    "            for i in range(data_shapes['temporal']):\n",
    "                # Extract single temporal feature\n",
    "                single_temporal = layers.Lambda(lambda x, idx=i: x[:, idx:idx+1])(temporal_input)\n",
    "                \n",
    "                if i == 0:  # Month\n",
    "                    emb = layers.Embedding(12, 8, name=f'month_embedding')(single_temporal)\n",
    "                    emb_dim = 8\n",
    "                elif i == 1:  # Quarter  \n",
    "                    emb = layers.Embedding(4, 4, name=f'quarter_embedding')(single_temporal)\n",
    "                    emb_dim = 4\n",
    "                else:\n",
    "                    emb = layers.Embedding(101, 8, name=f'temporal_{i}_embedding')(single_temporal)\n",
    "                    emb_dim = 8\n",
    "                \n",
    "                emb_flat = layers.Flatten()(emb)\n",
    "                temporal_embeddings.append(emb_flat)\n",
    "                total_embedding_dim += emb_dim\n",
    "            \n",
    "            if len(temporal_embeddings) > 1:\n",
    "                temporal_combined = layers.Concatenate(name='temporal_combined')(temporal_embeddings)\n",
    "            else:\n",
    "                temporal_combined = temporal_embeddings[0]\n",
    "            \n",
    "            embedding_outputs.append(temporal_combined)\n",
    "            print(f\"  Temporal embeddings: {len(temporal_embeddings)} features, total dim: {sum([8 if i==0 else 4 if i==1 else 8 for i in range(data_shapes['temporal'])])}\")\n",
    "        \n",
    "        # Continuous feature embeddings\n",
    "        if 'continuous' in data_shapes:\n",
    "            continuous_input = layers.Input(shape=(data_shapes['continuous'],), name='continuous_input')\n",
    "            inputs['continuous'] = continuous_input\n",
    "            \n",
    "            # Process each continuous feature with smaller embeddings\n",
    "            continuous_embeddings = []\n",
    "            embedding_dim_per_feature = 8  # Smaller dimension\n",
    "            \n",
    "            for i in range(data_shapes['continuous']):\n",
    "                single_continuous = layers.Lambda(lambda x, idx=i: x[:, idx:idx+1])(continuous_input)\n",
    "                emb = layers.Embedding(52, embedding_dim_per_feature, name=f'continuous_{i}_embedding')(single_continuous)\n",
    "                emb_flat = layers.Flatten()(emb)\n",
    "                continuous_embeddings.append(emb_flat)\n",
    "                total_embedding_dim += embedding_dim_per_feature\n",
    "            \n",
    "            if len(continuous_embeddings) > 1:\n",
    "                continuous_combined = layers.Concatenate(name='continuous_combined')(continuous_embeddings)\n",
    "            else:\n",
    "                continuous_combined = continuous_embeddings[0]\n",
    "            \n",
    "            embedding_outputs.append(continuous_combined)\n",
    "            print(f\"  Continuous embeddings: {len(continuous_embeddings)} features, total dim: {len(continuous_embeddings) * embedding_dim_per_feature}\")\n",
    "        \n",
    "        # Direct numerical features\n",
    "        direct_dim = 0\n",
    "        if 'direct' in data_shapes:\n",
    "            direct_input = layers.Input(shape=(data_shapes['direct'],), name='direct_input')\n",
    "            inputs['direct'] = direct_input\n",
    "            \n",
    "            # Process direct features to fixed dimension\n",
    "            direct_processed = layers.Dense(32, activation='relu', name='direct_dense')(direct_input)\n",
    "            direct_processed = layers.BatchNormalization(name='direct_bn')(direct_processed)\n",
    "            direct_processed = layers.Dropout(0.2, name='direct_dropout')(direct_processed)\n",
    "            \n",
    "            embedding_outputs.append(direct_processed)\n",
    "            direct_dim = 32\n",
    "            total_embedding_dim += direct_dim\n",
    "            print(f\"  Direct features: {data_shapes['direct']} → {direct_dim} dimensions\")\n",
    "        \n",
    "        # Calculate actual combined dimension\n",
    "        print(f\"  Expected total embedding dimension: {total_embedding_dim}\")\n",
    "        \n",
    "        # Combine all embeddings\n",
    "        if len(embedding_outputs) > 1:\n",
    "            combined = layers.Concatenate(name='combine_all')(embedding_outputs)\n",
    "        else:\n",
    "            combined = embedding_outputs[0]\n",
    "        \n",
    "        # Adaptive standardization - use actual input dimension\n",
    "        standardized = layers.Dense(256, activation='relu', name='standardize')(combined)\n",
    "        standardized = layers.BatchNormalization(name='std_bn')(standardized)\n",
    "        standardized = layers.Dropout(0.3, name='std_dropout')(standardized)\n",
    "        \n",
    "        # Multi-head attention with smaller heads\n",
    "        attention_1 = layers.Dense(64, activation='tanh', name='attention_1')(standardized)\n",
    "        attention_2 = layers.Dense(64, activation='tanh', name='attention_2')(standardized)\n",
    "        attention_3 = layers.Dense(64, activation='tanh', name='attention_3')(standardized)\n",
    "        attention_4 = layers.Dense(64, activation='tanh', name='attention_4')(standardized)\n",
    "        \n",
    "        multi_head = layers.Concatenate(name='multi_head')([attention_1, attention_2, attention_3, attention_4])\n",
    "        \n",
    "        # Residual connection - both inputs now 256-dim\n",
    "        attended = layers.Add(name='residual_attention')([standardized, multi_head])\n",
    "        attended = layers.LayerNormalization(name='layer_norm')(attended)\n",
    "        \n",
    "        # Deep layers\n",
    "        x1 = layers.Dense(256, activation='relu', name='deep1')(attended)\n",
    "        x1 = layers.BatchNormalization(name='bn1')(x1)\n",
    "        x1 = layers.Dropout(0.3, name='drop1')(x1)\n",
    "        \n",
    "        x2 = layers.Dense(128, activation='relu', name='deep2')(x1)\n",
    "        x2 = layers.BatchNormalization(name='bn2')(x2)\n",
    "        x2 = layers.Dropout(0.2, name='drop2')(x2)\n",
    "        \n",
    "        x3 = layers.Dense(64, activation='relu', name='deep3')(x2)\n",
    "        x3 = layers.Dropout(0.2, name='drop3')(x3)\n",
    "        \n",
    "        # Output\n",
    "        output = layers.Dense(1, activation='linear', name='sales_prediction')(x3)\n",
    "        \n",
    "        model = Model(inputs=list(inputs.values()), outputs=output, name='AdvancedEmbeddingModel')\n",
    "        \n",
    "        print(f\"  Model created with {model.count_params():,} parameters\")\n",
    "        print(f\"  Input types: {list(inputs.keys())}\")\n",
    "        \n",
    "        return model, list(inputs.keys())\n",
    "    \n",
    "    def enhanced_sanity_check_results(self, results):\n",
    "        \"\"\"Enhanced sanity checks on results\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"ENHANCED SANITY CHECKS ON RESULTS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"❌ No results to check\")\n",
    "            return False\n",
    "        \n",
    "        mapes = [result['mape'] for result in results.values()]\n",
    "        avg_mape = np.mean(mapes)\n",
    "        \n",
    "        # Check 1: Too good to be true?\n",
    "        if avg_mape < 5:\n",
    "            print(f\"🚨 SUSPICIOUS: Average MAPE ({avg_mape:.2f}%) is suspiciously low\")\n",
    "            print(\"   This may indicate data leakage or incorrect calculation\")\n",
    "            return False\n",
    "        \n",
    "        # Check 2: All splits performing similarly well?\n",
    "        mape_std = np.std(mapes)\n",
    "        if mape_std < 2 and avg_mape < 15:\n",
    "            print(f\"🚨 SUSPICIOUS: All splits perform very similarly ({mape_std:.2f}% std)\")\n",
    "            print(\"   This may indicate overfitting or data leakage\")\n",
    "            return False\n",
    "        \n",
    "        # Check 3: Check for perfect predictions across splits\n",
    "        total_perfect = sum([result.get('perfect_predictions', 0) for result in results.values()])\n",
    "        total_predictions = sum([result.get('total_predictions', 1) for result in results.values()])\n",
    "        perfect_ratio = total_perfect / total_predictions\n",
    "        \n",
    "        if perfect_ratio > 0.1:  # More than 10% perfect predictions\n",
    "            print(f\"🚨 SUSPICIOUS: {perfect_ratio*100:.1f}% perfect predictions across all splits\")\n",
    "            print(\"   This strongly suggests data leakage\")\n",
    "            return False\n",
    "        \n",
    "        # Check 4: Dramatic improvement from baseline?\n",
    "        if avg_mape < 10:\n",
    "            print(f\"📊 BUSINESS REALITY CHECK:\")\n",
    "            print(f\"   Average MAPE: {avg_mape:.2f}%\")\n",
    "            print(f\"   This means predictions are typically within {avg_mape:.1f}% of actual sales\")\n",
    "            print(f\"   For a product selling 1000 units, predictions would be ±{avg_mape*10:.0f} units\")\n",
    "            print(f\"   Please validate this level of accuracy with business stakeholders\")\n",
    "        \n",
    "        print(f\"✅ Results pass enhanced sanity checks\")\n",
    "        return True\n",
    "    \n",
    "    def train_advanced_embedding_model(self, df, features, rolling_splits):\n",
    "        \"\"\"Train advanced embedding model on rolling splits with prediction saving\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"TRAINING ADVANCED EMBEDDING-BASED MODELS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Analyze features\n",
    "        feature_categories = self.categorize_features_for_embeddings(df, features)\n",
    "        \n",
    "        all_results = {}\n",
    "        timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        for split_idx, (train_data, val_data, description) in enumerate(rolling_splits):\n",
    "            print(f\"\\nSplit {split_idx + 1}: {description}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            try:\n",
    "                # VALIDATE DATA INTEGRITY FIRST\n",
    "                print(f\"DATA INTEGRITY CHECKS:\")\n",
    "                print(f\"  Train date range: {train_data['sales_month'].min()} to {train_data['sales_month'].max()}\")\n",
    "                print(f\"  Val date range: {val_data['sales_month'].min()} to {val_data['sales_month'].max()}\")\n",
    "                \n",
    "                # Check for temporal overlap\n",
    "                train_max_date = train_data['sales_month'].max()\n",
    "                val_min_date = val_data['sales_month'].min()\n",
    "                \n",
    "                if train_max_date >= val_min_date:\n",
    "                    print(f\"  🚨 POTENTIAL DATA LEAKAGE: Training data overlaps with validation!\")\n",
    "                    print(f\"    Train max: {train_max_date}, Val min: {val_min_date}\")\n",
    "                \n",
    "                # Check for identical records\n",
    "                train_key = train_data[['store_name', 'brand_name', 'sales_month', 'sales_quantity']].copy()\n",
    "                val_key = val_data[['store_name', 'brand_name', 'sales_month', 'sales_quantity']].copy()\n",
    "                \n",
    "                # Create composite keys\n",
    "                train_key['composite'] = train_key['store_name'] + '_' + train_key['brand_name'] + '_' + train_key['sales_month'].astype(str)\n",
    "                val_key['composite'] = val_key['store_name'] + '_' + val_key['brand_name'] + '_' + val_key['sales_month'].astype(str)\n",
    "                \n",
    "                overlapping_keys = set(train_key['composite']).intersection(set(val_key['composite']))\n",
    "                \n",
    "                if overlapping_keys:\n",
    "                    print(f\"  🚨 IDENTICAL RECORDS DETECTED: {len(overlapping_keys)} records appear in both train and validation!\")\n",
    "                else:\n",
    "                    print(f\"  ✓ No identical records between train and validation\")\n",
    "                \n",
    "                # Check target distribution\n",
    "                train_sales_stats = train_data['sales_quantity'].describe()\n",
    "                val_sales_stats = val_data['sales_quantity'].describe()\n",
    "                \n",
    "                print(f\"  Train sales stats: mean={train_sales_stats['mean']:.0f}, std={train_sales_stats['std']:.0f}\")\n",
    "                print(f\"  Val sales stats: mean={val_sales_stats['mean']:.0f}, std={val_sales_stats['std']:.0f}\")\n",
    "                \n",
    "                # Prepare data\n",
    "                X_train, y_train = self.prepare_embedding_features(train_data, feature_categories, is_training=True)\n",
    "                X_val, y_val = self.prepare_embedding_features(val_data, feature_categories, is_training=False)\n",
    "                \n",
    "                print(f\"Prepared {len(X_train)} input types for training\")\n",
    "                \n",
    "                # Get data shapes for model creation\n",
    "                data_shapes = {key: data.shape[1] for key, data in X_train.items()}\n",
    "                print(f\"Data shapes: {data_shapes}\")\n",
    "                \n",
    "                # Create model\n",
    "                model, input_order = self.create_advanced_embedding_model(feature_categories, data_shapes)\n",
    "                \n",
    "                # Compile with consistent metrics\n",
    "                model.compile(\n",
    "                    optimizer=AdamW(learning_rate=0.001, weight_decay=0.01),\n",
    "                    loss='mae',\n",
    "                    metrics=[mape_metric_original_scale, rmse_metric_original_scale]\n",
    "                )\n",
    "                \n",
    "                # Prepare inputs in correct order\n",
    "                X_train_ordered = [X_train[key] for key in input_order if key in X_train]\n",
    "                X_val_ordered = [X_val[key] for key in input_order if key in X_val]\n",
    "                \n",
    "                # Callbacks\n",
    "                callbacks_list = [\n",
    "                    callbacks.EarlyStopping(\n",
    "                        patience=20,\n",
    "                        restore_best_weights=True,\n",
    "                        monitor='val_mape_metric_original_scale',\n",
    "                        mode='min'\n",
    "                    ),\n",
    "                    callbacks.ReduceLROnPlateau(\n",
    "                        patience=10,\n",
    "                        factor=0.5,\n",
    "                        monitor='val_mape_metric_original_scale',\n",
    "                        mode='min'\n",
    "                    )\n",
    "                ]\n",
    "                \n",
    "                # Train\n",
    "                print(\"Training advanced embedding model...\")\n",
    "                history = model.fit(\n",
    "                    X_train_ordered, y_train,\n",
    "                    validation_data=(X_val_ordered, y_val),\n",
    "                    epochs=100,\n",
    "                    batch_size=512,\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose=1 if split_idx == 0 else 0\n",
    "                )\n",
    "                \n",
    "                # Evaluate with detailed analysis\n",
    "                val_pred_log = model.predict(X_val_ordered, verbose=0)\n",
    "                \n",
    "                # Detailed debugging\n",
    "                print(f\"\\nDETAILED EVALUATION ANALYSIS:\")\n",
    "                print(f\"  Predictions (log space): [{val_pred_log.min():.3f}, {val_pred_log.max():.3f}]\")\n",
    "                \n",
    "                # Convert to original scale\n",
    "                val_pred_orig = np.expm1(val_pred_log.flatten())\n",
    "                val_true_orig = np.expm1(y_val)\n",
    "                \n",
    "                print(f\"  Predictions (original): [{val_pred_orig.min():.0f}, {val_pred_orig.max():.0f}] units\")\n",
    "                print(f\"  Actuals (original): [{val_true_orig.min():.0f}, {val_true_orig.max():.0f}] units\")\n",
    "                \n",
    "                # Check for potential issues\n",
    "                zero_predictions = np.sum(val_pred_orig < 1)\n",
    "                extreme_predictions = np.sum(val_pred_orig > 100000)\n",
    "                \n",
    "                print(f\"  Zero/near-zero predictions: {zero_predictions}/{len(val_pred_orig)} ({zero_predictions/len(val_pred_orig)*100:.1f}%)\")\n",
    "                print(f\"  Extreme predictions (>100K): {extreme_predictions}/{len(val_pred_orig)} ({extreme_predictions/len(val_pred_orig)*100:.1f}%)\")\n",
    "                \n",
    "                # Sample comparison\n",
    "                print(f\"  Sample comparisons (first 10):\")\n",
    "                for i in range(min(10, len(val_pred_orig))):\n",
    "                    actual = val_true_orig[i]\n",
    "                    predicted = val_pred_orig[i]\n",
    "                    error = abs(actual - predicted) / (actual + 1) * 100\n",
    "                    print(f\"    Actual: {actual:8.0f}, Predicted: {predicted:8.0f}, APE: {error:6.1f}%\")\n",
    "                \n",
    "                # Calculate MAPE step by step\n",
    "                raw_ape = np.abs(val_true_orig - val_pred_orig) / (val_true_orig + 1.0)\n",
    "                raw_mape = np.mean(raw_ape) * 100\n",
    "                \n",
    "                print(f\"  Raw MAPE calculation: {raw_mape:.2f}%\")\n",
    "                print(f\"  APE distribution: [{np.min(raw_ape)*100:.1f}%, {np.median(raw_ape)*100:.1f}%, {np.max(raw_ape)*100:.1f}%] (min/median/max)\")\n",
    "                \n",
    "                # Use safe MAPE calculation\n",
    "                mape = self.safe_mape_calculation(y_val, val_pred_log.flatten())\n",
    "                \n",
    "                # Compare with sklearn MAPE\n",
    "                try:\n",
    "                    sklearn_mape = mean_absolute_percentage_error(val_true_orig, val_pred_orig) * 100\n",
    "                    print(f\"  Sklearn MAPE: {sklearn_mape:.2f}%\")\n",
    "                    print(f\"  Our MAPE: {mape:.2f}%\")\n",
    "                    \n",
    "                    if abs(sklearn_mape - mape) > 5:\n",
    "                        print(f\"  ⚠️ MAPE calculation discrepancy detected!\")\n",
    "                except:\n",
    "                    print(f\"  Could not calculate sklearn MAPE\")\n",
    "                \n",
    "                # Check for data leakage indicators\n",
    "                perfect_predictions = np.sum(np.abs(val_true_orig - val_pred_orig) < 1)\n",
    "                print(f\"  Perfect/near-perfect predictions: {perfect_predictions}/{len(val_pred_orig)} ({perfect_predictions/len(val_pred_orig)*100:.1f}%)\")\n",
    "                \n",
    "                if perfect_predictions > len(val_pred_orig) * 0.1:  # More than 10% perfect\n",
    "                    print(f\"  🚨 POTENTIAL DATA LEAKAGE: Too many perfect predictions!\")\n",
    "                \n",
    "                if mape < 5:\n",
    "                    print(f\"  🚨 SUSPICIOUSLY LOW MAPE: Results may indicate data leakage!\")\n",
    "                \n",
    "                # Additional validation metrics\n",
    "                val_pred_clipped = np.clip(val_pred_orig, 1, 1e6)\n",
    "                val_true_clipped = np.clip(val_true_orig, 1, 1e6)\n",
    "                \n",
    "                rmse = np.sqrt(mean_squared_error(val_true_clipped, val_pred_clipped))\n",
    "                r2 = r2_score(val_true_clipped, val_pred_clipped)\n",
    "                mae = np.mean(np.abs(val_true_clipped - val_pred_clipped))\n",
    "                \n",
    "                print(f\"  RMSE: {rmse:,.0f}\")\n",
    "                print(f\"  MAE: {mae:,.0f}\")\n",
    "                print(f\"  R²: {r2:.4f}\")\n",
    "                \n",
    "                # Save model for inspection\n",
    "                model_filename = f\"advanced_embedding_model_split_{split_idx+1}_{timestamp}.h5\"\n",
    "                model.save(model_filename)\n",
    "                print(f\"  Model saved as: {model_filename}\")\n",
    "                \n",
    "                # SAVE DETAILED PREDICTIONS AND ANALYSIS\n",
    "                print(f\"\\nSAVING PREDICTIONS AND ANALYSIS:\")\n",
    "                results_df, saved_files = self.save_detailed_predictions(\n",
    "                    val_data, val_pred_orig, val_true_orig, \n",
    "                    split_idx+1, description, \"AdvancedEmbedding\", timestamp\n",
    "                )\n",
    "                \n",
    "                analysis_file = self.save_split_analysis_report(\n",
    "                    results_df, split_idx+1, description, \"AdvancedEmbedding\", timestamp\n",
    "                )\n",
    "                \n",
    "                # Get training metrics\n",
    "                final_val_mape = history.history.get('val_mape_metric_original_scale', [None])[-1]\n",
    "                \n",
    "                print(f\"\\nFINAL METRICS:\")\n",
    "                print(f\"  Training MAPE: {final_val_mape:.2f}%\" if final_val_mape else \"  Training MAPE: N/A\")\n",
    "                print(f\"  Evaluation MAPE: {mape:.2f}%\")\n",
    "                \n",
    "                if final_val_mape and abs(final_val_mape - mape) > 2:\n",
    "                    print(f\"  ⚠️ Training vs Evaluation inconsistency: {abs(final_val_mape - mape):.2f}% difference\")\n",
    "                \n",
    "                # Store comprehensive results\n",
    "                all_results[f'split_{split_idx+1}'] = {\n",
    "                    'description': description,\n",
    "                    'mape': mape,\n",
    "                    'train_mape': final_val_mape,\n",
    "                    'perfect_predictions': perfect_predictions,\n",
    "                    'total_predictions': len(val_pred_orig),\n",
    "                    'rmse': rmse,\n",
    "                    'r2': r2,\n",
    "                    'mae': mae,\n",
    "                    'saved_files': {\n",
    "                        'model': model_filename,\n",
    "                        'detailed_predictions': saved_files['detailed_predictions'],\n",
    "                        'summary_predictions': saved_files['summary_predictions'],\n",
    "                        'analysis_report': analysis_file\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Early exit if performance is poor\n",
    "                if mape > 500:\n",
    "                    print(\"Model performing poorly, trying next split...\")\n",
    "                    continue\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in split {split_idx + 1}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        # Print final results\n",
    "        self.print_final_results(all_results)\n",
    "        return all_results\n",
    "    \n",
    "    def print_final_results(self, results):\n",
    "        \"\"\"Print comprehensive results with saved files information\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"ADVANCED EMBEDDING MODEL RESULTS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"❌ No successful training completed\")\n",
    "            return\n",
    "        \n",
    "        mapes = [result['mape'] for result in results.values()]\n",
    "        avg_mape = np.mean(mapes)\n",
    "        \n",
    "        print(f\"Results by split:\")\n",
    "        for split_name, result in results.items():\n",
    "            train_mape = result.get('train_mape', 'N/A')\n",
    "            perfect_preds = result.get('perfect_predictions', 0)\n",
    "            total_preds = result.get('total_predictions', 0)\n",
    "            perfect_pct = (perfect_preds / total_preds * 100) if total_preds > 0 else 0\n",
    "            \n",
    "            print(f\"  {result['description']}:\")\n",
    "            print(f\"    MAPE: {result['mape']:.2f}% (train: {train_mape:.2f}% if train_mape != 'N/A' else 'N/A')\")\n",
    "            print(f\"    Perfect predictions: {perfect_preds:,}/{total_preds:,} ({perfect_pct:.1f}%)\")\n",
    "            \n",
    "            # Show saved files\n",
    "            if 'saved_files' in result:\n",
    "                files = result['saved_files']\n",
    "                print(f\"    📁 Saved files:\")\n",
    "                print(f\"      Model: {files.get('model', 'N/A')}\")\n",
    "                print(f\"      Predictions: {files.get('summary_predictions', 'N/A')}\")\n",
    "                print(f\"      Analysis: {files.get('analysis_report', 'N/A')}\")\n",
    "        \n",
    "        print(f\"\\nOverall Performance:\")\n",
    "        print(f\"  Average MAPE: {avg_mape:.2f}%\")\n",
    "        print(f\"  Best MAPE: {min(mapes):.2f}%\")\n",
    "        print(f\"  Worst MAPE: {max(mapes):.2f}%\")\n",
    "        print(f\"  Standard Deviation: {np.std(mapes):.2f}%\")\n",
    "        \n",
    "        # Enhanced sanity checks\n",
    "        total_perfect = sum([result.get('perfect_predictions', 0) for result in results.values()])\n",
    "        total_predictions = sum([result.get('total_predictions', 1) for result in results.values()])\n",
    "        overall_perfect_pct = (total_perfect / total_predictions * 100) if total_predictions > 0 else 0\n",
    "        \n",
    "        print(f\"\\nOVERALL QUALITY ANALYSIS:\")\n",
    "        print(f\"  Total predictions across all splits: {total_predictions:,}\")\n",
    "        print(f\"  Total perfect predictions: {total_perfect:,} ({overall_perfect_pct:.1f}%)\")\n",
    "        \n",
    "        # Perform enhanced sanity checks\n",
    "        is_sane = self.enhanced_sanity_check_results(results)\n",
    "        \n",
    "        if is_sane:\n",
    "            if avg_mape <= 20:\n",
    "                print(f\"\\n🎉 BREAKTHROUGH! Average MAPE ({avg_mape:.2f}%) broke 20% barrier!\")\n",
    "                if avg_mape <= 10:\n",
    "                    print(f\"🌟 EXCELLENT! Achieved business-usable accuracy!\")\n",
    "                    print(f\"📋 RECOMMENDATION: Validate these results with business stakeholders\")\n",
    "                    print(f\"📋 Review saved prediction files for detailed analysis\")\n",
    "            else:\n",
    "                print(f\"\\n⚠️ Still above 20% threshold ({avg_mape:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"\\n🚨 RESULTS FAILED SANITY CHECKS - INVESTIGATE POTENTIAL ISSUES\")\n",
    "            print(f\"   📋 RECOMMENDATION: Review saved prediction files to identify issues\")\n",
    "            print(f\"   📋 Check for data leakage in detailed prediction analysis\")\n",
    "        \n",
    "        # Summary of all saved files\n",
    "        print(f\"\\n📁 ALL SAVED FILES SUMMARY:\")\n",
    "        all_files = []\n",
    "        for result in results.values():\n",
    "            if 'saved_files' in result:\n",
    "                files = result['saved_files']\n",
    "                all_files.extend([\n",
    "                    files.get('model', ''),\n",
    "                    files.get('detailed_predictions', ''),\n",
    "                    files.get('summary_predictions', ''),\n",
    "                    files.get('analysis_report', '')\n",
    "                ])\n",
    "        \n",
    "        valid_files = [f for f in all_files if f and f != 'N/A']\n",
    "        if valid_files:\n",
    "            print(f\"  Total files saved: {len(valid_files)}\")\n",
    "            print(f\"  File types: Models, Predictions (detailed & summary), Analysis reports\")\n",
    "            print(f\"  Use these files for:\")\n",
    "            print(f\"    - Business validation of results\")\n",
    "            print(f\"    - Identifying data leakage patterns\")\n",
    "            print(f\"    - Understanding model performance by platform/brand/store\")\n",
    "            print(f\"    - Generating business insights and recommendations\")\n",
    "        else:\n",
    "            print(f\"  No files were saved\")\n",
    "\n",
    "# Initialize framework\n",
    "advanced_framework = AdvancedEmbeddingModel()\n",
    "\n",
    "print(\"\\nAdvanced Embedding Framework Ready!\")\n",
    "print(\"Run: results = advanced_framework.train_advanced_embedding_model(df_final, features, rolling_splits)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = advanced_framework.train_advanced_embedding_model(df_final, features, rolling_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mingyin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
